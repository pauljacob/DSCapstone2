{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project aims to train a classification model that can predict if a user will accept a coupon given his/her answers to some survey questions. In this notebook, we will explore the feature engineering methods for categorical data using sklearn's pipeline and workflow.\n",
    "\n",
    "The attributes of the data set include:\n",
    "\n",
    "This data was collected via a survey on Amazon Mechanical Turk. The survey describes different driving scenarios including the destination, current time, weather, passenger, etc., and then ask the person whether he will accept the coupon if he is the driver. For more information about the dataset, please refer to the paper: \n",
    "Wang, Tong, Cynthia Rudin, Finale Doshi-Velez, Yimin Liu, Erica Klampfl, and Perry MacNeille. 'A bayesian framework for learning rule sets for interpretable classification.' The Journal of Machine Learning Research 18, no. 1 (2017): 2357-2393.\n",
    "\n",
    "**User attributes**\n",
    "* gender: male, female\n",
    "* age: 21, 46, 26, 31, 41, 50plus, 36, below21 (e.g. 21 means 21 to 26, 26 means 26 to 31)\n",
    "* maritalStatus: Unmarried partner, Single, Married partner, Divorced, Widowed \n",
    "* has_Children:1, 0 \n",
    "* education: Some college - no degree, Bachelors degree, Associates degree, High School Graduate, Graduate degree (Masters or Doctorate), Some High School\n",
    "* occupation: Unemployed, Architecture & Engineering, Student, Education&Training&Library, Healthcare Support, Healthcare Practitioners & Technical, Sales & Related, Management, Arts Design Entertainment Sports & Media, Computer & Mathematical, Life Physical Social Science, Personal Care & Service, Community & Social Services, Office & Administrative Support, Construction & Extraction, Legal, Retired, Installation Maintenance & Repair, Transportation & Material Moving, Business & Financial, Protective Service, Food Preparation & Serving Related, Production Occupations, Building & Grounds Cleaning & Maintenance, Farming Fishing & Forestry \n",
    "* income: \\\\$37500 - \\\\$49999, \\\\$62500 - \\\\$74999, \\\\$12500 - \\\\$24999, \\\\$75000 - \\\\$87499, \\\\$50000 - \\\\$62499, \\\\$25000 - \\\\$37499, \\\\$100000 or More, \\\\$87500 - \\\\$99999, Less than \\\\$12500 \n",
    "* Bar: never, less1, 1\\~3, gt8, nan4\\~8 (feature meaning: how many times do you go to a bar every month?) \n",
    "* CoffeeHouse: never, less1, 4\\~8, 1\\~3, gt8, nan (feature meaning: how many times do you go to a coffeehouse every month?)\n",
    "* CarryAway:n4\\~8, 1\\~3, gt8, less1, never (feature meaning: how many times do you get take-away food every month?) \n",
    "* RestaurantLessThan20: 4\\~8, 1\\~3, less1, gt8, never (feature meaning: how many times do you go to a restaurant with an average expense per person of less than \\\\$20 every month?) \n",
    "* Restaurant20To50: 1\\~3, less1, never, gt8, 4\\~8, nan (feature meaning: how many times do you go to a restaurant with average expense per person of \\\\$20 - \\\\$50 every month?) \n",
    "\n",
    "**Contextual attributes**\n",
    "* destination: No Urgent Place, Home, Work \n",
    "* passanger: Alone, Friend(s), Kid(s), Partner (who are the passengers in the car)\n",
    "* weather: Sunny, Rainy, Snowy \n",
    "* temperature:55, 80, 30 \n",
    "* time: 2PM, 10AM, 6PM, 7AM, 10PM \n",
    "* toCoupon_GEQ5min:0, 1 (feature meaning: driving distance to the restaurant/bar for using the coupon is greater than 5 minutes) \n",
    "* toCoupon_GEQ15min:0,1 (feature meaning: driving distance to the restaurant/bar for using the coupon is greater than 15 minutes) \n",
    "* toCoupon_GEQ25min:0, 1 (feature meaning: driving distance to the restaurant/bar for using the coupon is greater than 25 minutes) \n",
    "* direction_same:0, 1 (feature meaning: whether the restaurant/bar is in the same direction as your current destination) \n",
    "* direction_opp:1, 0 (feature meaning: whether the restaurant/bar is in the same direction as your current destination) \n",
    "\n",
    "**Coupon attributes**\n",
    "* coupon: Restaurant(<\\\\$20), Coffee House, Carry out & Take away, Bar, Restaurant(\\\\$20-\\\\$50) \n",
    "* expiration: 1d, 2h (the coupon expires in 1 day or in 2 hours) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering of this task can be divided into two parts. The first part is data wrangling that can be done on the dataset as a whole. The second part is operating some feature engineerings after split the data into traning and testing set. This section will also be filled with Exploratory Data Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-07-03T00:54:48.404723Z",
     "iopub.status.busy": "2021-07-03T00:54:48.404048Z",
     "iopub.status.idle": "2021-07-03T00:55:08.468849Z",
     "shell.execute_reply": "2021-07-03T00:55:08.467770Z",
     "shell.execute_reply.started": "2021-07-03T00:54:48.404625Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datapane in /opt/anaconda3/lib/python3.9/site-packages (0.15.3)\n",
      "Requirement already satisfied: datacommons-pandas<0.0.4,>=0.0.3 in /opt/anaconda3/lib/python3.9/site-packages (from datapane) (0.0.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.0 in /opt/anaconda3/lib/python3.9/site-packages (from datapane) (8.0.3)\n",
      "Requirement already satisfied: colorlog<7.0.0,>=4.1.0 in /opt/anaconda3/lib/python3.9/site-packages (from datapane) (6.6.0)\n",
      "Requirement already satisfied: pyarrow<7.0.0,>=3.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from datapane) (6.0.1)\n",
      "Requirement already satisfied: boltons<22.0.0,>=20.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from datapane) (21.0.0)\n",
      "Requirement already satisfied: requests-toolbelt<0.10.0,>=0.9.1 in /opt/anaconda3/lib/python3.9/site-packages (from datapane) (0.9.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.19.0 in /opt/anaconda3/lib/python3.9/site-packages (from datapane) (2.26.0)\n",
      "Requirement already satisfied: altair<5.0.0,>=4.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from datapane) (4.2.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.9/site-packages (from datapane) (3.2.0)\n",
      "Requirement already satisfied: dominate<3.0.0,>=2.4.0 in /opt/anaconda3/lib/python3.9/site-packages (from datapane) (2.7.0)\n",
      "Requirement already satisfied: datacommons<2.0.0,>=1.4.3 in /opt/anaconda3/lib/python3.9/site-packages (from datapane) (1.4.3)\n",
      "Requirement already satisfied: pandas<2.0.0,>=1.1.0 in /opt/anaconda3/lib/python3.9/site-packages (from datapane) (1.3.4)\n",
      "Requirement already satisfied: glom<23.0.0,>=20.11.0 in /opt/anaconda3/lib/python3.9/site-packages (from datapane) (22.1.0)\n",
      "Requirement already satisfied: importlib_resources<6.0.0,>=3.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from datapane) (5.9.0)\n",
      "Requirement already satisfied: toolz<0.13.0,>=0.11.0 in /opt/anaconda3/lib/python3.9/site-packages (from datapane) (0.11.1)\n",
      "Requirement already satisfied: pydantic<2.0.0,>=1.6.0 in /opt/anaconda3/lib/python3.9/site-packages (from datapane) (1.10.2)\n",
      "Requirement already satisfied: tabulate<0.9.0,>=0.8.0 in /opt/anaconda3/lib/python3.9/site-packages (from datapane) (0.8.9)\n",
      "Requirement already satisfied: chardet<6.0.0,>=4.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from datapane) (4.0.0)\n",
      "Requirement already satisfied: validators<0.21.0,>=0.18.0 in /opt/anaconda3/lib/python3.9/site-packages (from datapane) (0.20.0)\n",
      "Requirement already satisfied: packaging<22.0.0,>=20.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from datapane) (21.0)\n",
      "Requirement already satisfied: dulwich<0.21.0,>=0.20.0 in /opt/anaconda3/lib/python3.9/site-packages (from datapane) (0.20.46)\n",
      "Requirement already satisfied: posthog<3.0.0,>=1.4.0 in /opt/anaconda3/lib/python3.9/site-packages (from datapane) (2.1.2)\n",
      "Requirement already satisfied: stringcase<2.0.0,>=1.2.0 in /opt/anaconda3/lib/python3.9/site-packages (from datapane) (1.2.0)\n",
      "Requirement already satisfied: Jinja2<4.0.0,>=3.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from datapane) (3.1.2)\n",
      "Requirement already satisfied: nbconvert<7.0.0,>=6.1.0 in /opt/anaconda3/lib/python3.9/site-packages (from datapane) (6.1.0)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.4.0 in /opt/anaconda3/lib/python3.9/site-packages (from datapane) (5.4.1)\n",
      "Requirement already satisfied: micawber>=0.5.0 in /opt/anaconda3/lib/python3.9/site-packages (from datapane) (0.5.4)\n",
      "Requirement already satisfied: furl<3.0.0,>=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from datapane) (2.1.3)\n",
      "Requirement already satisfied: dacite<2.0.0,>=1.0.2 in /opt/anaconda3/lib/python3.9/site-packages (from datapane) (1.6.0)\n",
      "Requirement already satisfied: munch<3.0.0,>=2.3.0 in /opt/anaconda3/lib/python3.9/site-packages (from datapane) (2.5.0)\n",
      "Requirement already satisfied: click-spinner<0.2.0,>=0.1.8 in /opt/anaconda3/lib/python3.9/site-packages (from datapane) (0.1.10)\n",
      "Requirement already satisfied: vega-datasets<1.0.0,>=0.9.0 in /opt/anaconda3/lib/python3.9/site-packages (from datapane) (0.9.0)\n",
      "Requirement already satisfied: lxml<5.0.0,>=4.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from datapane) (4.6.3)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.9/site-packages (from altair<5.0.0,>=4.0.0->datapane) (1.20.3)\n",
      "Requirement already satisfied: entrypoints in /opt/anaconda3/lib/python3.9/site-packages (from altair<5.0.0,>=4.0.0->datapane) (0.3)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.9/site-packages (from datacommons<2.0.0,>=1.4.3->datapane) (1.16.0)\n",
      "Requirement already satisfied: urllib3>=1.25 in /opt/anaconda3/lib/python3.9/site-packages (from dulwich<0.21.0,>=0.20.0->datapane) (1.26.7)\n",
      "Requirement already satisfied: orderedmultidict>=1.0.1 in /opt/anaconda3/lib/python3.9/site-packages (from furl<3.0.0,>=2.0.0->datapane) (1.0.1)\n",
      "Requirement already satisfied: face>=20.1.0 in /opt/anaconda3/lib/python3.9/site-packages (from glom<23.0.0,>=20.11.0->datapane) (20.1.1)\n",
      "Requirement already satisfied: attrs in /opt/anaconda3/lib/python3.9/site-packages (from glom<23.0.0,>=20.11.0->datapane) (21.2.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/anaconda3/lib/python3.9/site-packages (from importlib_resources<6.0.0,>=3.0.0->datapane) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.9/site-packages (from Jinja2<4.0.0,>=3.0.0->datapane) (2.1.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.9/site-packages (from jsonschema<5.0.0,>=3.2.0->datapane) (59.6.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/anaconda3/lib/python3.9/site-packages (from jsonschema<5.0.0,>=3.2.0->datapane) (0.18.0)\n",
      "Requirement already satisfied: bleach in /opt/anaconda3/lib/python3.9/site-packages (from nbconvert<7.0.0,>=6.1.0->datapane) (4.0.0)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/anaconda3/lib/python3.9/site-packages (from nbconvert<7.0.0,>=6.1.0->datapane) (0.5.3)\n",
      "Requirement already satisfied: pygments>=2.4.1 in /opt/anaconda3/lib/python3.9/site-packages (from nbconvert<7.0.0,>=6.1.0->datapane) (2.10.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/anaconda3/lib/python3.9/site-packages (from nbconvert<7.0.0,>=6.1.0->datapane) (0.8.4)\n",
      "Requirement already satisfied: nbformat>=4.4 in /opt/anaconda3/lib/python3.9/site-packages (from nbconvert<7.0.0,>=6.1.0->datapane) (5.1.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/anaconda3/lib/python3.9/site-packages (from nbconvert<7.0.0,>=6.1.0->datapane) (1.4.3)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/anaconda3/lib/python3.9/site-packages (from nbconvert<7.0.0,>=6.1.0->datapane) (0.1.2)\n",
      "Requirement already satisfied: defusedxml in /opt/anaconda3/lib/python3.9/site-packages (from nbconvert<7.0.0,>=6.1.0->datapane) (0.7.1)\n",
      "Requirement already satisfied: jupyter-core in /opt/anaconda3/lib/python3.9/site-packages (from nbconvert<7.0.0,>=6.1.0->datapane) (4.8.1)\n",
      "Requirement already satisfied: traitlets>=5.0 in /opt/anaconda3/lib/python3.9/site-packages (from nbconvert<7.0.0,>=6.1.0->datapane) (5.1.0)\n",
      "Requirement already satisfied: testpath in /opt/anaconda3/lib/python3.9/site-packages (from nbconvert<7.0.0,>=6.1.0->datapane) (0.5.0)\n",
      "Requirement already satisfied: async-generator in /opt/anaconda3/lib/python3.9/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert<7.0.0,>=6.1.0->datapane) (1.10)\n",
      "Requirement already satisfied: nest-asyncio in /opt/anaconda3/lib/python3.9/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert<7.0.0,>=6.1.0->datapane) (1.5.1)\n",
      "Requirement already satisfied: jupyter-client>=6.1.5 in /opt/anaconda3/lib/python3.9/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert<7.0.0,>=6.1.0->datapane) (6.1.12)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/anaconda3/lib/python3.9/site-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert<7.0.0,>=6.1.0->datapane) (2.8.2)\n",
      "Requirement already satisfied: pyzmq>=13 in /opt/anaconda3/lib/python3.9/site-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert<7.0.0,>=6.1.0->datapane) (22.2.1)\n",
      "Requirement already satisfied: tornado>=4.1 in /opt/anaconda3/lib/python3.9/site-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert<7.0.0,>=6.1.0->datapane) (6.1)\n",
      "Requirement already satisfied: ipython-genutils in /opt/anaconda3/lib/python3.9/site-packages (from nbformat>=4.4->nbconvert<7.0.0,>=6.1.0->datapane) (0.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/anaconda3/lib/python3.9/site-packages (from packaging<22.0.0,>=20.0.0->datapane) (3.0.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/anaconda3/lib/python3.9/site-packages (from pandas<2.0.0,>=1.1.0->datapane) (2021.3)\n",
      "Requirement already satisfied: monotonic>=1.5 in /opt/anaconda3/lib/python3.9/site-packages (from posthog<3.0.0,>=1.4.0->datapane) (1.6)\n",
      "Requirement already satisfied: backoff<2.0.0,>=1.10.0 in /opt/anaconda3/lib/python3.9/site-packages (from posthog<3.0.0,>=1.4.0->datapane) (1.11.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /opt/anaconda3/lib/python3.9/site-packages (from pydantic<2.0.0,>=1.6.0->datapane) (4.3.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.19.0->datapane) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.19.0->datapane) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.19.0->datapane) (2021.10.8)\n",
      "Requirement already satisfied: decorator>=3.4.0 in /opt/anaconda3/lib/python3.9/site-packages (from validators<0.21.0,>=0.18.0->datapane) (5.1.0)\n",
      "Requirement already satisfied: webencodings in /opt/anaconda3/lib/python3.9/site-packages (from bleach->nbconvert<7.0.0,>=6.1.0->datapane) (0.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install datapane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:55:08.471779Z",
     "iopub.status.busy": "2021-07-03T00:55:08.471478Z",
     "iopub.status.idle": "2021-07-03T00:55:10.300960Z",
     "shell.execute_reply": "2021-07-03T00:55:10.300115Z",
     "shell.execute_reply.started": "2021-07-03T00:55:08.471741Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import datapane as dp\n",
    "import sklearn\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import base\n",
    "from collections import defaultdict\n",
    "from matplotlib.ticker import FixedLocator, FixedFormatter\n",
    "from joblib import dump, load\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.utils.fixes import loguniform\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, \\\n",
    "RandomizedSearchCV, cross_val_score, RepeatedStratifiedKFold, KFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement scikitplot (from versions: none)\u001b[0m\r\n",
      "\u001b[31mERROR: No matching distribution found for scikitplot\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install scikitplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scikitplot as skplt\n",
    "# import missingno as msno\n",
    "# from kmodes.kprototypes import KPrototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:55:10.302265Z",
     "iopub.status.busy": "2021-07-03T00:55:10.301991Z",
     "iopub.status.idle": "2021-07-03T00:55:14.237502Z",
     "shell.execute_reply": "2021-07-03T00:55:14.236456Z",
     "shell.execute_reply.started": "2021-07-03T00:55:10.302237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mConnected successfully to https://cloud.datapane.com as ZXiao7@my.harrisburgu.edu\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!datapane login --token=ea0597866d4a1147ca57892a8536b4b7ffc47bcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:55:14.239262Z",
     "iopub.status.busy": "2021-07-03T00:55:14.238924Z",
     "iopub.status.idle": "2021-07-03T00:55:14.243800Z",
     "shell.execute_reply": "2021-07-03T00:55:14.242928Z",
     "shell.execute_reply.started": "2021-07-03T00:55:14.239229Z"
    }
   },
   "outputs": [],
   "source": [
    "# set random seed for reproducibility\n",
    "RANDOM_SEED = 2021\n",
    "os.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:55:14.246536Z",
     "iopub.status.busy": "2021-07-03T00:55:14.246126Z",
     "iopub.status.idle": "2021-07-03T00:55:14.259503Z",
     "shell.execute_reply": "2021-07-03T00:55:14.258579Z",
     "shell.execute_reply.started": "2021-07-03T00:55:14.246505Z"
    }
   },
   "outputs": [],
   "source": [
    "# to better display long string\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:55:14.262879Z",
     "iopub.status.busy": "2021-07-03T00:55:14.262403Z",
     "iopub.status.idle": "2021-07-03T00:55:14.378826Z",
     "shell.execute_reply": "2021-07-03T00:55:14.377947Z",
     "shell.execute_reply.started": "2021-07-03T00:55:14.262834Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join('..', 'data', 'raw', 'in-vehicle-coupon-recommendation.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:55:14.380433Z",
     "iopub.status.busy": "2021-07-03T00:55:14.380154Z",
     "iopub.status.idle": "2021-07-03T00:55:14.390932Z",
     "shell.execute_reply": "2021-07-03T00:55:14.389947Z",
     "shell.execute_reply.started": "2021-07-03T00:55:14.380407Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ultimate Data Processing Function\n",
    "def data_cleaning(data=df):\n",
    "    clean_df = df.copy()\n",
    "    \n",
    "    clean_df.drop(columns=['car'], inplace=True)\n",
    "    na_columns = ['Bar', 'CoffeeHouse', 'CarryAway', 'RestaurantLessThan20', 'Restaurant20To50']\n",
    "    clean_df.drop(columns=['toCoupon_GEQ5min'], inplace=True)\n",
    "    frequency_map = {\n",
    "        'never': 0,\n",
    "        'less1': 1,\n",
    "        '1~3': 2,\n",
    "        '4~8': 3,\n",
    "        'gt8': 4}\n",
    "    age_map = {\n",
    "        'below21': 0,\n",
    "        '21': 1,\n",
    "        '26': 2,\n",
    "        '31': 3,\n",
    "        '36': 4,\n",
    "        '41': 5,\n",
    "        '46': 6,\n",
    "        '50plus': 7}\n",
    "    income_map = {\n",
    "        'Less than $12500': 0,\n",
    "        '$12500 - $24999': 1,\n",
    "        '$25000 - $37499': 2,\n",
    "        '$37500 - $49999': 3,\n",
    "        '$50000 - $62499': 4,\n",
    "        '$62500 - $74999': 5,\n",
    "        '$75000 - $87499': 6,\n",
    "        '$87500 - $99999': 7,\n",
    "        '$100000 or More': 8}\n",
    "    frequency_cols = ['Restaurant20To50', 'RestaurantLessThan20', \n",
    "                      'CarryAway', 'CoffeeHouse', 'Bar']\n",
    "    for col in frequency_cols:\n",
    "        clean_df[col] = clean_df[col].map(frequency_map)\n",
    "    clean_df.age = clean_df.age.map(age_map)\n",
    "    clean_df.income = clean_df.income.map(income_map)\n",
    "    clean_df.drop(columns=['direction_opp'], inplace=True)\n",
    "    clean_df['distance'] = None\n",
    "    clean_df.loc[clean_df['toCoupon_GEQ15min'] == 0, 'distance'] = 0\n",
    "    clean_df.loc[(clean_df['toCoupon_GEQ15min'] == 1) & \\\n",
    "                 (clean_df['toCoupon_GEQ25min'] == 0), 'distance'] = 1\n",
    "    clean_df.loc[clean_df['toCoupon_GEQ25min'] == 1, 'distance'] = 2\n",
    "    clean_df.distance = clean_df.distance.astype('int64')\n",
    "    clean_df.drop(columns=['toCoupon_GEQ15min', 'toCoupon_GEQ25min'], inplace=True)\n",
    "    clean_df.has_children = clean_df.has_children.astype(str)\n",
    "    clean_df.direction_same = clean_df.direction_same.astype(str)\n",
    "    return clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:55:14.392707Z",
     "iopub.status.busy": "2021-07-03T00:55:14.392407Z",
     "iopub.status.idle": "2021-07-03T00:55:14.405926Z",
     "shell.execute_reply": "2021-07-03T00:55:14.404874Z",
     "shell.execute_reply.started": "2021-07-03T00:55:14.392677Z"
    }
   },
   "outputs": [],
   "source": [
    "# # use data_clean then jump right to the model training part\n",
    "# clean_df = data_cleaning(df)\n",
    "# clean_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def initialize_custom_notebook_settings():\n",
    "    '''\n",
    "    initialize the jupyter notebook display width'''\n",
    "        \n",
    "    from IPython.core.display import display, HTML\n",
    "    display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "    \n",
    "    pd.options.display.max_columns = 999\n",
    "    pd.options.display.max_rows = 999\n",
    "\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    pd.options.display.max_info_columns = 999\n",
    "\n",
    "initialize_custom_notebook_settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:55:14.407731Z",
     "iopub.status.busy": "2021-07-03T00:55:14.407361Z",
     "iopub.status.idle": "2021-07-03T00:55:14.457514Z",
     "shell.execute_reply": "2021-07-03T00:55:14.456446Z",
     "shell.execute_reply.started": "2021-07-03T00:55:14.407697Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>destination</th>\n",
       "      <th>passanger</th>\n",
       "      <th>weather</th>\n",
       "      <th>temperature</th>\n",
       "      <th>time</th>\n",
       "      <th>coupon</th>\n",
       "      <th>expiration</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>maritalStatus</th>\n",
       "      <th>has_children</th>\n",
       "      <th>education</th>\n",
       "      <th>occupation</th>\n",
       "      <th>income</th>\n",
       "      <th>Bar</th>\n",
       "      <th>CoffeeHouse</th>\n",
       "      <th>CarryAway</th>\n",
       "      <th>RestaurantLessThan20</th>\n",
       "      <th>Restaurant20To50</th>\n",
       "      <th>toCoupon_GEQ5min</th>\n",
       "      <th>toCoupon_GEQ15min</th>\n",
       "      <th>toCoupon_GEQ25min</th>\n",
       "      <th>direction_same</th>\n",
       "      <th>direction_opp</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Urgent Place</td>\n",
       "      <td>Alone</td>\n",
       "      <td>Sunny</td>\n",
       "      <td>55</td>\n",
       "      <td>2PM</td>\n",
       "      <td>Restaurant(&lt;20)</td>\n",
       "      <td>1d</td>\n",
       "      <td>Female</td>\n",
       "      <td>21</td>\n",
       "      <td>Unmarried partner</td>\n",
       "      <td>1</td>\n",
       "      <td>Some college - no degree</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>$37500 - $49999</td>\n",
       "      <td>never</td>\n",
       "      <td>never</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4~8</td>\n",
       "      <td>1~3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No Urgent Place</td>\n",
       "      <td>Friend(s)</td>\n",
       "      <td>Sunny</td>\n",
       "      <td>80</td>\n",
       "      <td>10AM</td>\n",
       "      <td>Coffee House</td>\n",
       "      <td>2h</td>\n",
       "      <td>Female</td>\n",
       "      <td>21</td>\n",
       "      <td>Unmarried partner</td>\n",
       "      <td>1</td>\n",
       "      <td>Some college - no degree</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>$37500 - $49999</td>\n",
       "      <td>never</td>\n",
       "      <td>never</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4~8</td>\n",
       "      <td>1~3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No Urgent Place</td>\n",
       "      <td>Friend(s)</td>\n",
       "      <td>Sunny</td>\n",
       "      <td>80</td>\n",
       "      <td>10AM</td>\n",
       "      <td>Carry out &amp; Take away</td>\n",
       "      <td>2h</td>\n",
       "      <td>Female</td>\n",
       "      <td>21</td>\n",
       "      <td>Unmarried partner</td>\n",
       "      <td>1</td>\n",
       "      <td>Some college - no degree</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>$37500 - $49999</td>\n",
       "      <td>never</td>\n",
       "      <td>never</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4~8</td>\n",
       "      <td>1~3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No Urgent Place</td>\n",
       "      <td>Friend(s)</td>\n",
       "      <td>Sunny</td>\n",
       "      <td>80</td>\n",
       "      <td>2PM</td>\n",
       "      <td>Coffee House</td>\n",
       "      <td>2h</td>\n",
       "      <td>Female</td>\n",
       "      <td>21</td>\n",
       "      <td>Unmarried partner</td>\n",
       "      <td>1</td>\n",
       "      <td>Some college - no degree</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>$37500 - $49999</td>\n",
       "      <td>never</td>\n",
       "      <td>never</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4~8</td>\n",
       "      <td>1~3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No Urgent Place</td>\n",
       "      <td>Friend(s)</td>\n",
       "      <td>Sunny</td>\n",
       "      <td>80</td>\n",
       "      <td>2PM</td>\n",
       "      <td>Coffee House</td>\n",
       "      <td>1d</td>\n",
       "      <td>Female</td>\n",
       "      <td>21</td>\n",
       "      <td>Unmarried partner</td>\n",
       "      <td>1</td>\n",
       "      <td>Some college - no degree</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>$37500 - $49999</td>\n",
       "      <td>never</td>\n",
       "      <td>never</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4~8</td>\n",
       "      <td>1~3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>No Urgent Place</td>\n",
       "      <td>Friend(s)</td>\n",
       "      <td>Sunny</td>\n",
       "      <td>80</td>\n",
       "      <td>6PM</td>\n",
       "      <td>Restaurant(&lt;20)</td>\n",
       "      <td>2h</td>\n",
       "      <td>Female</td>\n",
       "      <td>21</td>\n",
       "      <td>Unmarried partner</td>\n",
       "      <td>1</td>\n",
       "      <td>Some college - no degree</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>$37500 - $49999</td>\n",
       "      <td>never</td>\n",
       "      <td>never</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4~8</td>\n",
       "      <td>1~3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>No Urgent Place</td>\n",
       "      <td>Friend(s)</td>\n",
       "      <td>Sunny</td>\n",
       "      <td>55</td>\n",
       "      <td>2PM</td>\n",
       "      <td>Carry out &amp; Take away</td>\n",
       "      <td>1d</td>\n",
       "      <td>Female</td>\n",
       "      <td>21</td>\n",
       "      <td>Unmarried partner</td>\n",
       "      <td>1</td>\n",
       "      <td>Some college - no degree</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>$37500 - $49999</td>\n",
       "      <td>never</td>\n",
       "      <td>never</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4~8</td>\n",
       "      <td>1~3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>No Urgent Place</td>\n",
       "      <td>Kid(s)</td>\n",
       "      <td>Sunny</td>\n",
       "      <td>80</td>\n",
       "      <td>10AM</td>\n",
       "      <td>Restaurant(&lt;20)</td>\n",
       "      <td>2h</td>\n",
       "      <td>Female</td>\n",
       "      <td>21</td>\n",
       "      <td>Unmarried partner</td>\n",
       "      <td>1</td>\n",
       "      <td>Some college - no degree</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>$37500 - $49999</td>\n",
       "      <td>never</td>\n",
       "      <td>never</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4~8</td>\n",
       "      <td>1~3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       destination  passanger weather  temperature  time  \\\n",
       "0  No Urgent Place      Alone   Sunny           55   2PM   \n",
       "1  No Urgent Place  Friend(s)   Sunny           80  10AM   \n",
       "2  No Urgent Place  Friend(s)   Sunny           80  10AM   \n",
       "3  No Urgent Place  Friend(s)   Sunny           80   2PM   \n",
       "4  No Urgent Place  Friend(s)   Sunny           80   2PM   \n",
       "5  No Urgent Place  Friend(s)   Sunny           80   6PM   \n",
       "6  No Urgent Place  Friend(s)   Sunny           55   2PM   \n",
       "7  No Urgent Place     Kid(s)   Sunny           80  10AM   \n",
       "\n",
       "                  coupon expiration  gender age      maritalStatus  \\\n",
       "0        Restaurant(<20)         1d  Female  21  Unmarried partner   \n",
       "1           Coffee House         2h  Female  21  Unmarried partner   \n",
       "2  Carry out & Take away         2h  Female  21  Unmarried partner   \n",
       "3           Coffee House         2h  Female  21  Unmarried partner   \n",
       "4           Coffee House         1d  Female  21  Unmarried partner   \n",
       "5        Restaurant(<20)         2h  Female  21  Unmarried partner   \n",
       "6  Carry out & Take away         1d  Female  21  Unmarried partner   \n",
       "7        Restaurant(<20)         2h  Female  21  Unmarried partner   \n",
       "\n",
       "   has_children                 education  occupation           income    Bar  \\\n",
       "0             1  Some college - no degree  Unemployed  $37500 - $49999  never   \n",
       "1             1  Some college - no degree  Unemployed  $37500 - $49999  never   \n",
       "2             1  Some college - no degree  Unemployed  $37500 - $49999  never   \n",
       "3             1  Some college - no degree  Unemployed  $37500 - $49999  never   \n",
       "4             1  Some college - no degree  Unemployed  $37500 - $49999  never   \n",
       "5             1  Some college - no degree  Unemployed  $37500 - $49999  never   \n",
       "6             1  Some college - no degree  Unemployed  $37500 - $49999  never   \n",
       "7             1  Some college - no degree  Unemployed  $37500 - $49999  never   \n",
       "\n",
       "  CoffeeHouse CarryAway RestaurantLessThan20 Restaurant20To50  \\\n",
       "0       never       NaN                  4~8              1~3   \n",
       "1       never       NaN                  4~8              1~3   \n",
       "2       never       NaN                  4~8              1~3   \n",
       "3       never       NaN                  4~8              1~3   \n",
       "4       never       NaN                  4~8              1~3   \n",
       "5       never       NaN                  4~8              1~3   \n",
       "6       never       NaN                  4~8              1~3   \n",
       "7       never       NaN                  4~8              1~3   \n",
       "\n",
       "   toCoupon_GEQ5min  toCoupon_GEQ15min  toCoupon_GEQ25min  direction_same  \\\n",
       "0                 1                  0                  0               0   \n",
       "1                 1                  0                  0               0   \n",
       "2                 1                  1                  0               0   \n",
       "3                 1                  1                  0               0   \n",
       "4                 1                  1                  0               0   \n",
       "5                 1                  1                  0               0   \n",
       "6                 1                  1                  0               0   \n",
       "7                 1                  1                  0               0   \n",
       "\n",
       "   direction_opp  Y  \n",
       "0              1  1  \n",
       "1              1  0  \n",
       "2              1  1  \n",
       "3              1  0  \n",
       "4              1  0  \n",
       "5              1  1  \n",
       "6              1  1  \n",
       "7              1  1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No Urgent Place    6283\n",
       "Home               3237\n",
       "Work               3164\n",
       "Name: destination, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, 'destination'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Alone        7305\n",
       "Friend(s)    3298\n",
       "Partner      1075\n",
       "Kid(s)       1006\n",
       "Name: passanger, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, 'passanger'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sunny    10069\n",
       "Snowy     1405\n",
       "Rainy     1210\n",
       "Name: weather, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, 'weather'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80    6528\n",
       "55    3840\n",
       "30    2316\n",
       "Name: temperature, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, 'temperature'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6PM     3230\n",
       "7AM     3164\n",
       "10AM    2275\n",
       "2PM     2009\n",
       "10PM    2006\n",
       "Name: time, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, 'time'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Coffee House             3996\n",
       "Restaurant(<20)          2786\n",
       "Carry out & Take away    2393\n",
       "Bar                      2017\n",
       "Restaurant(20-50)        1492\n",
       "Name: coupon, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, 'coupon'].value_counts() #######???????????? coupon type\n",
    "\n",
    "#five types of coupons:\n",
    "#bars\n",
    "#takeaway food restaurants\n",
    "#coffee houses\n",
    "#cheap restaurants (average expense below $20 per person)\n",
    "#expensive restaurants (average expense between $20 to $50 per person)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1d    7091\n",
       "2h    5593\n",
       "Name: expiration, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, 'expiration'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Female    6511\n",
       "Male      6173\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, 'gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21         2653\n",
       "26         2559\n",
       "31         2039\n",
       "50plus     1788\n",
       "36         1319\n",
       "41         1093\n",
       "46          686\n",
       "below21     547\n",
       "Name: age, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, 'age'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Married partner      5100\n",
       "Single               4752\n",
       "Unmarried partner    2186\n",
       "Divorced              516\n",
       "Widowed               130\n",
       "Name: maritalStatus, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, 'maritalStatus'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7431\n",
       "1    5253\n",
       "Name: has_children, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, 'has_children'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Some college - no degree                  4351\n",
       "Bachelors degree                          4335\n",
       "Graduate degree (Masters or Doctorate)    1852\n",
       "Associates degree                         1153\n",
       "High School Graduate                       905\n",
       "Some High School                            88\n",
       "Name: education, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, 'education'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unemployed                                   1870\n",
       "Student                                      1584\n",
       "Computer & Mathematical                      1408\n",
       "Sales & Related                              1093\n",
       "Education&Training&Library                    943\n",
       "Management                                    838\n",
       "Office & Administrative Support               639\n",
       "Arts Design Entertainment Sports & Media      629\n",
       "Business & Financial                          544\n",
       "Retired                                       495\n",
       "Food Preparation & Serving Related            298\n",
       "Healthcare Practitioners & Technical          244\n",
       "Healthcare Support                            242\n",
       "Community & Social Services                   241\n",
       "Legal                                         219\n",
       "Transportation & Material Moving              218\n",
       "Architecture & Engineering                    175\n",
       "Personal Care & Service                       175\n",
       "Protective Service                            175\n",
       "Life Physical Social Science                  170\n",
       "Construction & Extraction                     154\n",
       "Installation Maintenance & Repair             133\n",
       "Production Occupations                        110\n",
       "Building & Grounds Cleaning & Maintenance      44\n",
       "Farming Fishing & Forestry                     43\n",
       "Name: occupation, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, 'occupation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$25000 - $37499     2013\n",
       "$12500 - $24999     1831\n",
       "$37500 - $49999     1805\n",
       "$100000 or More     1736\n",
       "$50000 - $62499     1659\n",
       "Less than $12500    1042\n",
       "$87500 - $99999      895\n",
       "$75000 - $87499      857\n",
       "$62500 - $74999      846\n",
       "Name: income, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, 'income'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "never    5197\n",
       "less1    3482\n",
       "1~3      2473\n",
       "4~8      1076\n",
       "gt8       349\n",
       "Name: Bar, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, 'Bar'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "less1    3385\n",
       "1~3      3225\n",
       "never    2962\n",
       "4~8      1784\n",
       "gt8      1111\n",
       "Name: CoffeeHouse, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, 'CoffeeHouse'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1~3      4672\n",
       "4~8      4258\n",
       "less1    1856\n",
       "gt8      1594\n",
       "never     153\n",
       "Name: CarryAway, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, 'CarryAway'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1~3      5376\n",
       "4~8      3580\n",
       "less1    2093\n",
       "gt8      1285\n",
       "never     220\n",
       "Name: RestaurantLessThan20, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, 'RestaurantLessThan20'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "less1    6077\n",
       "1~3      3290\n",
       "never    2136\n",
       "4~8       728\n",
       "gt8       264\n",
       "Name: Restaurant20To50, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, 'Restaurant20To50'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    12684\n",
       "Name: toCoupon_GEQ5min, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, 'toCoupon_GEQ5min'].value_counts()    ##########?????????????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    7122\n",
       "0    5562\n",
       "Name: toCoupon_GEQ15min, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, 'toCoupon_GEQ15min'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    11173\n",
       "1     1511\n",
       "Name: toCoupon_GEQ25min, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, 'toCoupon_GEQ25min'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    9960\n",
       "1    2724\n",
       "Name: direction_same, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, 'direction_same'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    9960\n",
       "0    2724\n",
       "Name: direction_opp, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, 'direction_opp'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    7210\n",
       "0    5474\n",
       "Name: Y, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, 'Y'].value_counts() #whether the coupon is accepted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:55:14.459315Z",
     "iopub.status.busy": "2021-07-03T00:55:14.458917Z",
     "iopub.status.idle": "2021-07-03T00:55:14.492327Z",
     "shell.execute_reply": "2021-07-03T00:55:14.491292Z",
     "shell.execute_reply.started": "2021-07-03T00:55:14.459280Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12684 entries, 0 to 12683\n",
      "Data columns (total 26 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   destination           12684 non-null  object\n",
      " 1   passanger             12684 non-null  object\n",
      " 2   weather               12684 non-null  object\n",
      " 3   temperature           12684 non-null  int64 \n",
      " 4   time                  12684 non-null  object\n",
      " 5   coupon                12684 non-null  object\n",
      " 6   expiration            12684 non-null  object\n",
      " 7   gender                12684 non-null  object\n",
      " 8   age                   12684 non-null  object\n",
      " 9   maritalStatus         12684 non-null  object\n",
      " 10  has_children          12684 non-null  int64 \n",
      " 11  education             12684 non-null  object\n",
      " 12  occupation            12684 non-null  object\n",
      " 13  income                12684 non-null  object\n",
      " 14  car                   108 non-null    object\n",
      " 15  Bar                   12577 non-null  object\n",
      " 16  CoffeeHouse           12467 non-null  object\n",
      " 17  CarryAway             12533 non-null  object\n",
      " 18  RestaurantLessThan20  12554 non-null  object\n",
      " 19  Restaurant20To50      12495 non-null  object\n",
      " 20  toCoupon_GEQ5min      12684 non-null  int64 \n",
      " 21  toCoupon_GEQ15min     12684 non-null  int64 \n",
      " 22  toCoupon_GEQ25min     12684 non-null  int64 \n",
      " 23  direction_same        12684 non-null  int64 \n",
      " 24  direction_opp         12684 non-null  int64 \n",
      " 25  Y                     12684 non-null  int64 \n",
      "dtypes: int64(8), object(18)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Car**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some missing values in several columns, and the 'car' variable has only 108 non-null values, more than 99% of the values are NaN. We can just drop it off. Before doing that, let's have a look of the non-null values in 'car' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:55:14.494485Z",
     "iopub.status.busy": "2021-07-03T00:55:14.494044Z",
     "iopub.status.idle": "2021-07-03T00:55:14.501256Z",
     "shell.execute_reply": "2021-07-03T00:55:14.500201Z",
     "shell.execute_reply.started": "2021-07-03T00:55:14.494442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan 'Scooter and motorcycle' 'crossover' 'Mazda5' 'do not drive'\n",
      " 'Car that is too old to install Onstar :D']\n"
     ]
    }
   ],
   "source": [
    "print(df.car.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No much information left in the 'car' variable, we can just drop it off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:55:14.502659Z",
     "iopub.status.busy": "2021-07-03T00:55:14.502399Z",
     "iopub.status.idle": "2021-07-03T00:55:14.516119Z",
     "shell.execute_reply": "2021-07-03T00:55:14.514970Z",
     "shell.execute_reply.started": "2021-07-03T00:55:14.502633Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['car'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, check missing values of other columns:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:55:14.517689Z",
     "iopub.status.busy": "2021-07-03T00:55:14.517300Z",
     "iopub.status.idle": "2021-07-03T00:55:14.541232Z",
     "shell.execute_reply": "2021-07-03T00:55:14.540205Z",
     "shell.execute_reply.started": "2021-07-03T00:55:14.517659Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "destination               0\n",
       "passanger                 0\n",
       "weather                   0\n",
       "temperature               0\n",
       "time                      0\n",
       "coupon                    0\n",
       "expiration                0\n",
       "gender                    0\n",
       "age                       0\n",
       "maritalStatus             0\n",
       "has_children              0\n",
       "education                 0\n",
       "occupation                0\n",
       "income                    0\n",
       "Bar                     107\n",
       "CoffeeHouse             217\n",
       "CarryAway               151\n",
       "RestaurantLessThan20    130\n",
       "Restaurant20To50        189\n",
       "toCoupon_GEQ5min          0\n",
       "toCoupon_GEQ15min         0\n",
       "toCoupon_GEQ25min         0\n",
       "direction_same            0\n",
       "direction_opp             0\n",
       "Y                         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are five more columns with missing values, and all of them have data type 'object', which means they are all strings. Time to do some missing value visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:55:14.542743Z",
     "iopub.status.busy": "2021-07-03T00:55:14.542495Z",
     "iopub.status.idle": "2021-07-03T00:55:14.904164Z",
     "shell.execute_reply": "2021-07-03T00:55:14.903104Z",
     "shell.execute_reply.started": "2021-07-03T00:55:14.542719Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'msno' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/77/y4hv4_s9751293nzv4yrfrtm0000gn/T/ipykernel_37293/2721932940.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mna_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Bar'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CoffeeHouse'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CarryAway'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'RestaurantLessThan20'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Restaurant20To50'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mna_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mna_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmsno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mna_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'msno' is not defined"
     ]
    }
   ],
   "source": [
    "na_columns = ['Bar', 'CoffeeHouse', 'CarryAway', 'RestaurantLessThan20', 'Restaurant20To50']\n",
    "na_df = df[na_columns]\n",
    "msno.matrix(na_df, figsize=(10, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this matrix, we can probably say the missing values are at least missing at random. Let's do a correlation heatmap to ensure that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:55:14.905754Z",
     "iopub.status.busy": "2021-07-03T00:55:14.905470Z",
     "iopub.status.idle": "2021-07-03T00:55:15.261903Z",
     "shell.execute_reply": "2021-07-03T00:55:15.260841Z",
     "shell.execute_reply.started": "2021-07-03T00:55:14.905724Z"
    }
   },
   "outputs": [],
   "source": [
    "msno.heatmap(na_df, figsize=(10, 8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, missing at random. So, we can do data imputation on them. Since we cannot directly impute data on the whole dataset, we are saving data imputation until we finished the data splitting. Before that, let's explore the dataset a little more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:55:15.265321Z",
     "iopub.status.busy": "2021-07-03T00:55:15.264945Z",
     "iopub.status.idle": "2021-07-03T00:55:15.424703Z",
     "shell.execute_reply": "2021-07-03T00:55:15.423669Z",
     "shell.execute_reply.started": "2021-07-03T00:55:15.265286Z"
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:55:15.426582Z",
     "iopub.status.busy": "2021-07-03T00:55:15.426290Z",
     "iopub.status.idle": "2021-07-03T00:55:15.442765Z",
     "shell.execute_reply": "2021-07-03T00:55:15.441592Z",
     "shell.execute_reply.started": "2021-07-03T00:55:15.426553Z"
    }
   },
   "outputs": [],
   "source": [
    "df.select_dtypes('int64').nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, the column 'toCoupon_GEQ5min' has only one single value: 1. No variance at all, so, we can drop it. According to the dataset description, this column means driving distance to the restaurant/bar for using the coupon is greater than 5 minutes, so all the restaurant/bars are at least five minutes away from the driver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**toCoupon_GEQ5min**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:55:15.444971Z",
     "iopub.status.busy": "2021-07-03T00:55:15.444340Z",
     "iopub.status.idle": "2021-07-03T00:55:15.453067Z",
     "shell.execute_reply": "2021-07-03T00:55:15.452139Z",
     "shell.execute_reply.started": "2021-07-03T00:55:15.444924Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['toCoupon_GEQ5min'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:55:15.454638Z",
     "iopub.status.busy": "2021-07-03T00:55:15.454361Z",
     "iopub.status.idle": "2021-07-03T00:55:15.501329Z",
     "shell.execute_reply": "2021-07-03T00:55:15.500328Z",
     "shell.execute_reply.started": "2021-07-03T00:55:15.454612Z"
    }
   },
   "outputs": [],
   "source": [
    "df.select_dtypes('object').nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:55:15.503307Z",
     "iopub.status.busy": "2021-07-03T00:55:15.502902Z",
     "iopub.status.idle": "2021-07-03T00:55:15.532769Z",
     "shell.execute_reply": "2021-07-03T00:55:15.531817Z",
     "shell.execute_reply.started": "2021-07-03T00:55:15.503265Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in df.select_dtypes('object').columns:\n",
    "    print(i, df[i].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many levels in the **occupation** column. If simply apply onehotencoder on it, it will greatly increase the sparsity of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have finished the basic data wrangling part, if other problems rises during exploratory data analysis, we can repeat the data wrangling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:55:15.534331Z",
     "iopub.status.busy": "2021-07-03T00:55:15.534071Z",
     "iopub.status.idle": "2021-07-03T00:55:15.539358Z",
     "shell.execute_reply": "2021-07-03T00:55:15.538218Z",
     "shell.execute_reply.started": "2021-07-03T00:55:15.534304Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_df = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we plot the count plot of each categorical variable:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:55:15.540877Z",
     "iopub.status.busy": "2021-07-03T00:55:15.540594Z",
     "iopub.status.idle": "2021-07-03T00:55:18.909507Z",
     "shell.execute_reply": "2021-07-03T00:55:18.907042Z",
     "shell.execute_reply.started": "2021-07-03T00:55:15.540851Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(9, 2, figsize=(20,50))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, col in zip(axes, clean_df.select_dtypes('object').columns):\n",
    "    sns.countplot(y=col, data=clean_df, ax=ax, \n",
    "                  palette=\"ch:.25\", order=clean_df[col].value_counts().index);\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the counting plot of the categorical features, we realized that there are two kinds of categorical data: ordinal and nominal, we can apply one hot encoder to nominal features, however, ordinal data should be mapped into numerical data so that the inner order can preserve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ordinal Features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those ordinal features are: \n",
    "\n",
    "* **Restaurant20To50**: how many times do you go to a restaurant with average expense per person of \\\\$20 - \\\\$50 every month?\n",
    "* **RestaurantLessThan20**: how many times do you go to a restaurant with an average expense per person of less than \\\\$20 every month?\n",
    "* **CarryAway**: how many times do you get take-away food every month?\n",
    "* **CoffeeHouse**: how many times do you go to a coffeehouse every month?\n",
    "* **age**: quite self-explanatory\n",
    "* **Bar**: how many times do you go to bar every month?\n",
    "* **income**: income range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's map them into numerical values with respect to their inner order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:55:18.911689Z",
     "iopub.status.busy": "2021-07-03T00:55:18.911215Z",
     "iopub.status.idle": "2021-07-03T00:55:18.917739Z",
     "shell.execute_reply": "2021-07-03T00:55:18.916527Z",
     "shell.execute_reply.started": "2021-07-03T00:55:18.911646Z"
    }
   },
   "outputs": [],
   "source": [
    "frequency_map = {\n",
    "    'never': 0,\n",
    "    'less1': 1,\n",
    "    '1~3': 2,\n",
    "    '4~8': 3,\n",
    "    'gt8': 4\n",
    "}\n",
    "age_map = {\n",
    "    'below21': 0,\n",
    "    '21': 1,\n",
    "    '26': 2,\n",
    "    '31': 3,\n",
    "    '36': 4,\n",
    "    '41': 5,\n",
    "    '46': 6,\n",
    "    '50plus': 7\n",
    "}\n",
    "income_map = {\n",
    "    'Less than $12500': 0,\n",
    "    '$12500 - $24999': 1,\n",
    "    '$25000 - $37499': 2,\n",
    "    '$37500 - $49999': 3,\n",
    "    '$50000 - $62499': 4,\n",
    "    '$62500 - $74999': 5,\n",
    "    '$75000 - $87499': 6,\n",
    "    '$87500 - $99999': 7,\n",
    "    '$100000 or More': 8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:55:18.919228Z",
     "iopub.status.busy": "2021-07-03T00:55:18.918915Z",
     "iopub.status.idle": "2021-07-03T00:55:18.932262Z",
     "shell.execute_reply": "2021-07-03T00:55:18.931079Z",
     "shell.execute_reply.started": "2021-07-03T00:55:18.919199Z"
    }
   },
   "outputs": [],
   "source": [
    "frequency_cols = ['Restaurant20To50', 'RestaurantLessThan20', \n",
    "                  'CarryAway', 'CoffeeHouse', 'Bar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:55:18.933999Z",
     "iopub.status.busy": "2021-07-03T00:55:18.933679Z",
     "iopub.status.idle": "2021-07-03T00:55:18.967157Z",
     "shell.execute_reply": "2021-07-03T00:55:18.966022Z",
     "shell.execute_reply.started": "2021-07-03T00:55:18.933971Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in frequency_cols:\n",
    "    clean_df[col] = clean_df[col].map(frequency_map)\n",
    "clean_df.age = clean_df.age.map(age_map)\n",
    "clean_df.income = clean_df.income.map(income_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nominal Features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's have a look of the nominal features:\n",
    "\n",
    "* **destination**: The destination of the driver\n",
    "* **passenger**: Type of passenger in the car\n",
    "* **weather**: Sunny, rainy, or snowy\n",
    "* **time**: Current time\n",
    "* **coupon**: Coupon type\n",
    "* **expiration**: Coupon expiration time\n",
    "* **gender**: Female or male\n",
    "* **maritalStatus**: Marital status of the driver\n",
    "* **occupation**: Occupation of the driver\n",
    "* **education** Education level of the driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put occupation aside, let's plot the relationships between those features and the target feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:55:18.968624Z",
     "iopub.status.busy": "2021-07-03T00:55:18.968364Z",
     "iopub.status.idle": "2021-07-03T00:55:20.636025Z",
     "shell.execute_reply": "2021-07-03T00:55:20.635002Z",
     "shell.execute_reply.started": "2021-07-03T00:55:18.968597Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(20, 15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, col in zip(axes, ['destination', 'passanger', 'weather', 'time', \n",
    "                          'coupon', 'expiration', 'gender', 'maritalStatus',\n",
    "                          'education']):\n",
    "    sns.countplot(y=col, hue='Y', data=clean_df, ax=ax, palette='ch:.25')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **destination** People that has no urgent place to go has a higher probability to accept the coupon.\n",
    "* **passenger** If the passengers in car are friends of the driver, they are more likely to accept the coupon.\n",
    "* **weather** People tend to accept the coupon when it is sunny.\n",
    "* **time** If the time is too early or too late, the probability of accepting the coupon is lower.\n",
    "* **coupon** If the coupon is of a coffee house, the probability of accepting the coupon is just the same as rejecting it. If the coupon is of a cheap restaurant or carry out, most people will accept the coupon. If the coupon is of a Bar of expensive Restaurant, people tend to refuse it.\n",
    "* **expiration** People are more likely to accept a coupon that expires in one day than one in two hours.\n",
    "* **gender** There is no much difference between gender.\n",
    "* **maritalStatus** Single people are most likely to accept the coupon.\n",
    "* **education** Some college, Bachelor or high school graduate are more likely to accept the coupon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at the occupation feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:55:20.638241Z",
     "iopub.status.busy": "2021-07-03T00:55:20.637567Z",
     "iopub.status.idle": "2021-07-03T00:55:21.123357Z",
     "shell.execute_reply": "2021-07-03T00:55:21.122339Z",
     "shell.execute_reply.started": "2021-07-03T00:55:20.638197Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(15, 20))\n",
    "sns.countplot(y ='occupation', hue='Y', data=clean_df, palette='ch:.25');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows some differences among jobs. \n",
    "\n",
    "As a result, we think all those nominal features are strong predictor variables. Thus, we will do frequency encoding and target encoding on them to improve their predictive power. Those two encodings will be done after data splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numerical Features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the correlations among all the numerical features now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:55:21.125534Z",
     "iopub.status.busy": "2021-07-03T00:55:21.124910Z",
     "iopub.status.idle": "2021-07-03T00:55:21.481795Z",
     "shell.execute_reply": "2021-07-03T00:55:21.480639Z",
     "shell.execute_reply.started": "2021-07-03T00:55:21.125489Z"
    }
   },
   "outputs": [],
   "source": [
    "cmap = sns.color_palette('vlag', as_cmap=True)\n",
    "sns.heatmap(clean_df.select_dtypes('int64', 'float64').corr(), cmap=cmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the heatmap shows, there are two columns sharing the same information: **direction_same** and **direction_opp**. They indicate whether the restaurant/bar is in the same direction as your current destination. So we decide to delete the **direction_opp** column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides, there are correlations among those frequency columns: **Bar, CoffeeHouse, CarryAway, RestaurantLessThan20, Restaurant20To50**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:55:21.483676Z",
     "iopub.status.busy": "2021-07-03T00:55:21.483240Z",
     "iopub.status.idle": "2021-07-03T00:55:21.491465Z",
     "shell.execute_reply": "2021-07-03T00:55:21.489987Z",
     "shell.execute_reply.started": "2021-07-03T00:55:21.483630Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_df.drop(columns=['direction_opp'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:55:21.493555Z",
     "iopub.status.busy": "2021-07-03T00:55:21.493154Z",
     "iopub.status.idle": "2021-07-03T00:55:23.346268Z",
     "shell.execute_reply": "2021-07-03T00:55:23.345220Z",
     "shell.execute_reply.started": "2021-07-03T00:55:21.493512Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_df.hist(figsize=(20, 15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**toCoupon_GEQ15min, toCoupon_GEQ25min**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After excluding the toCoupon_GEDQ5min feature, we see there are still two features about the driving distance to the Coupon's location: **toCoupon_GEQ15min** and **toCoupon_GEQ25min**. They have two possible values: 0 and 1 indicating yes or no to the question: is the driving distance to the restaurant/bar for using the coupon is greater than 15 minutes/25 minutes? We should be able to combine these two columns into one with ordinal data inside: greater than 5 minutes and less than 15 minutes, greater than 15 minutes and less than 25 minutes, and greater than 25 minutes. We believe doing that should be better than treat those two columns using onehotencoder just like different categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:55:23.348834Z",
     "iopub.status.busy": "2021-07-03T00:55:23.348365Z",
     "iopub.status.idle": "2021-07-03T00:55:23.367512Z",
     "shell.execute_reply": "2021-07-03T00:55:23.366435Z",
     "shell.execute_reply.started": "2021-07-03T00:55:23.348785Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_df['distance'] = None\n",
    "clean_df.loc[clean_df['toCoupon_GEQ15min'] == 0, 'distance'] = 0\n",
    "clean_df.loc[(clean_df['toCoupon_GEQ15min'] == 1) & \\\n",
    "             (clean_df['toCoupon_GEQ25min'] == 0), 'distance'] = 1\n",
    "clean_df.loc[clean_df['toCoupon_GEQ25min'] == 1, 'distance'] = 2\n",
    "clean_df.distance.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:55:23.376494Z",
     "iopub.status.busy": "2021-07-03T00:55:23.376144Z",
     "iopub.status.idle": "2021-07-03T00:55:23.384940Z",
     "shell.execute_reply": "2021-07-03T00:55:23.383942Z",
     "shell.execute_reply.started": "2021-07-03T00:55:23.376461Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_df.distance = clean_df.distance.astype('int64')\n",
    "clean_df.distance.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:55:23.386890Z",
     "iopub.status.busy": "2021-07-03T00:55:23.386618Z",
     "iopub.status.idle": "2021-07-03T00:55:23.401122Z",
     "shell.execute_reply": "2021-07-03T00:55:23.400147Z",
     "shell.execute_reply.started": "2021-07-03T00:55:23.386864Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_df.distance.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can drop these two columns: toCoupon_GEQ15min and toCoupon_GEQ25min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:55:23.402867Z",
     "iopub.status.busy": "2021-07-03T00:55:23.402571Z",
     "iopub.status.idle": "2021-07-03T00:55:23.415683Z",
     "shell.execute_reply": "2021-07-03T00:55:23.414620Z",
     "shell.execute_reply.started": "2021-07-03T00:55:23.402839Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_df.drop(columns=['toCoupon_GEQ15min', 'toCoupon_GEQ25min'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:55:23.417176Z",
     "iopub.status.busy": "2021-07-03T00:55:23.416867Z",
     "iopub.status.idle": "2021-07-03T00:55:23.441009Z",
     "shell.execute_reply": "2021-07-03T00:55:23.439132Z",
     "shell.execute_reply.started": "2021-07-03T00:55:23.417146Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some columns needed to be transformed into categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:55:23.442595Z",
     "iopub.status.busy": "2021-07-03T00:55:23.442231Z",
     "iopub.status.idle": "2021-07-03T00:55:23.466351Z",
     "shell.execute_reply": "2021-07-03T00:55:23.465021Z",
     "shell.execute_reply.started": "2021-07-03T00:55:23.442554Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_df.has_children = clean_df.has_children.astype(str)\n",
    "clean_df.direction_same = clean_df.direction_same.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look of the target feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:55:23.468011Z",
     "iopub.status.busy": "2021-07-03T00:55:23.467706Z",
     "iopub.status.idle": "2021-07-03T00:55:23.475636Z",
     "shell.execute_reply": "2021-07-03T00:55:23.474722Z",
     "shell.execute_reply.started": "2021-07-03T00:55:23.467983Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_df.Y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's well balanced. Accuracy can be a good metric for model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore relationships between some of the features and the target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:55:23.477114Z",
     "iopub.status.busy": "2021-07-03T00:55:23.476685Z",
     "iopub.status.idle": "2021-07-03T00:55:23.855100Z",
     "shell.execute_reply": "2021-07-03T00:55:23.854206Z",
     "shell.execute_reply.started": "2021-07-03T00:55:23.477073Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, col in zip(axes, ['distance', 'temperature']):\n",
    "    sns.countplot(x=col, hue='Y', data=clean_df, \n",
    "                  ax=ax, palette=\"ch:.25\");\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows some subtle, positive correlations between short distance, high temperature and the probability to accept the coupon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot the correlations between numerical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:55:23.856587Z",
     "iopub.status.busy": "2021-07-03T00:55:23.856165Z",
     "iopub.status.idle": "2021-07-03T00:56:26.503315Z",
     "shell.execute_reply": "2021-07-03T00:56:26.502302Z",
     "shell.execute_reply.started": "2021-07-03T00:55:23.856557Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.pairplot(clean_df, hue='Y', diag_kind='hist')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pair plot tells us one thing: how sparse our dataset is. All the numerical variables are discrete numbers, and actually categorical. If the numerical representation did not give a good result, we can treat all the variables as categorical and train models again.\n",
    "\n",
    "If we look into the hist plot, we can see that there are more people accepting coupon than rejecting when temperature is high and when distance is short. It means temperature and distance are decent predictive variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:56:26.504698Z",
     "iopub.status.busy": "2021-07-03T00:56:26.504432Z",
     "iopub.status.idle": "2021-07-03T00:56:26.509848Z",
     "shell.execute_reply": "2021-07-03T00:56:26.509131Z",
     "shell.execute_reply.started": "2021-07-03T00:56:26.504672Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with 21 features, we are ready to train classification models on the cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:56:26.511153Z",
     "iopub.status.busy": "2021-07-03T00:56:26.510878Z",
     "iopub.status.idle": "2021-07-03T00:56:26.611421Z",
     "shell.execute_reply": "2021-07-03T00:56:26.610351Z",
     "shell.execute_reply.started": "2021-07-03T00:56:26.511106Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the cleaned data set\n",
    "clean_df.to_csv('clean_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:56:26.612831Z",
     "iopub.status.busy": "2021-07-03T00:56:26.612565Z",
     "iopub.status.idle": "2021-07-03T00:56:26.616430Z",
     "shell.execute_reply": "2021-07-03T00:56:26.615408Z",
     "shell.execute_reply.started": "2021-07-03T00:56:26.612805Z"
    }
   },
   "outputs": [],
   "source": [
    "# Notice the values of have_children and direction_same are 0 and 1\n",
    "# Thus, if not specified, the pd.read_csv() method will infer them as integer\n",
    "# clean_df = pd.read_csv('/kaggle/working/clean_df.csv', \n",
    "#                        dtype={'has_children': str,\n",
    "#                              'direction_same': str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the exploratory data analysis, we have almost all categorical variables. There are two sections of feature engineering we saved that can only be done with splitted data: imputation and categorical encoding.\n",
    "\n",
    "* Data Imputation\n",
    "\n",
    "We will use simple imputer to replace the missing value with the most frequent item.\n",
    "\n",
    "* Categorical Encoding\n",
    "\n",
    "For those categorical features we believe are strong predictors, we will do both frequency encoding and target encoding. For other categorical features, OneHotEncoder will be applied.\n",
    "\n",
    "**Further Feature Engineering**\n",
    "\n",
    "There are other considerations about the feature engineering:\n",
    "* do we need to apply dimension reduction?\n",
    "\n",
    "After the OneHotEncoding, we can predict the number of features to reach 50 or more, and the frequency columns are correlated, so it is possible to apply dimension reduction methods like PCA. \n",
    "* do we need to apply feature expansion?\n",
    "\n",
    "We can use clustering method to add labels into the dataset as a new feature to help the classification task.\n",
    "\n",
    "Those two considerations will be experimented in the model training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Splitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:56:26.617664Z",
     "iopub.status.busy": "2021-07-03T00:56:26.617414Z",
     "iopub.status.idle": "2021-07-03T00:56:26.637256Z",
     "shell.execute_reply": "2021-07-03T00:56:26.635985Z",
     "shell.execute_reply.started": "2021-07-03T00:56:26.617639Z"
    }
   },
   "outputs": [],
   "source": [
    "X = clean_df.drop(columns=['Y'])\n",
    "y = clean_df.Y\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(X, y, random_state=RANDOM_SEED, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Two Plans**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we want to compare the effects of different feature engineering: \n",
    "* **plan A** is to do frequency and target encoding for strong predictors we observed in EDA part, and OneHotEncoding for other categorical features. \n",
    "* **Plan B** is to apply OneHotEncoding for all the categorical features.\n",
    "\n",
    "We will then compare the predictive performances of these two plans and decide which plan to keep for further tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plan A**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:56:26.639271Z",
     "iopub.status.busy": "2021-07-03T00:56:26.638654Z",
     "iopub.status.idle": "2021-07-03T00:56:26.646200Z",
     "shell.execute_reply": "2021-07-03T00:56:26.645341Z",
     "shell.execute_reply.started": "2021-07-03T00:56:26.639227Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_a = X_train.copy()\n",
    "X_test_a = X_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Frequency Encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Here](https://www.kaggle.com/bhavikapanara/frequency-encoding) is the reference for frequency encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:56:26.648030Z",
     "iopub.status.busy": "2021-07-03T00:56:26.647626Z",
     "iopub.status.idle": "2021-07-03T00:56:27.152283Z",
     "shell.execute_reply": "2021-07-03T00:56:27.151348Z",
     "shell.execute_reply.started": "2021-07-03T00:56:26.647988Z"
    }
   },
   "outputs": [],
   "source": [
    "strong_predictors = ['destination', 'passanger', 'weather', 'time', 'coupon',\n",
    "            'expiration', 'maritalStatus', 'education',\n",
    "            'occupation', 'direction_same']\n",
    "for col in strong_predictors:\n",
    "    # create frequency encoder\n",
    "    freq_encoder = X_train_a.groupby(col).size() / len(X_train_a)\n",
    "    # fit_transform for X_train\n",
    "    X_train_a[col + '_freq'] = X_train_a[col].apply(lambda x: freq_encoder[x])\n",
    "    # transform for X_test\n",
    "    X_test_a[col + '_freq'] = X_test_a[col].apply(lambda x: freq_encoder[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-fold Target Encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is borrowed from Pourya from his medium blog: [K-Fold Target Encoding](https://medium.com/@pouryaayria/k-fold-target-encoding-dfe9a594874b). We added some docstrings and comments to make the code more readable. H2O has a great package named `TargetEncoder` which has basically the same workflow as Pourya's code, the tutorial can be found [here](http://h2o-release.s3.amazonaws.com/h2o/rel-yates/4/docs-website/h2o-docs/data-munging/target-encoding.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:56:27.153782Z",
     "iopub.status.busy": "2021-07-03T00:56:27.153529Z",
     "iopub.status.idle": "2021-07-03T00:56:27.164958Z",
     "shell.execute_reply": "2021-07-03T00:56:27.164137Z",
     "shell.execute_reply.started": "2021-07-03T00:56:27.153741Z"
    }
   },
   "outputs": [],
   "source": [
    "class KFoldTargetEncoderTrain(base.BaseEstimator, base.TransformerMixin):\n",
    "    \"\"\"\n",
    "    This object contains a target encoder for a training set which should have\n",
    "    both X and y. \n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    feature:          string. Name of the feature in the training set.\n",
    "    target:           string. Name of the target in the training set.\n",
    "    n_fold:           default 5. Number of folds to use in KFold.\n",
    "    verbose:          bool, default True. If set to True, the correlation between the \n",
    "                      feature and the target will be calculated and printed out.\n",
    "    discard_original: bool,, default False. If set to True, the feature column will be \n",
    "                      deleted from the training set.\n",
    "                      \n",
    "    Example\n",
    "    ---------\n",
    "    train_target_encoder = KFoldTargetEncoderTrain(feature='A', target='target')\n",
    "    new_train = train_target_encoder.fit_transform(train)\n",
    "    \"\"\"\n",
    "    def __init__(self, feature, target, n_fold=5, verbose=True, discard_original=False):\n",
    "\n",
    "        self.feature = feature\n",
    "        self.target = target\n",
    "        self.n_fold = n_fold\n",
    "        self.verbose = verbose\n",
    "        self.discard_original = discard_original\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,X):\n",
    "        \"\"\"\n",
    "        Transform the original training set. Notice this function can only encode \n",
    "        one feature once.\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "        X: A pandas DataFrame which should include both the feature and the target.\n",
    "        \n",
    "        Output:\n",
    "        X: A pandas DataFrame with the target encoding.\n",
    "        \"\"\"\n",
    "        \n",
    "        # notice this function can only encode one feature at a time\n",
    "        assert(type(self.feature) == str)\n",
    "        assert(type(self.target) == str)\n",
    "        assert(self.feature in X.columns)\n",
    "        assert(self.target in X.columns)\n",
    "\n",
    "        mean_of_target = X[self.target].mean()\n",
    "        kf = KFold(n_splits = self.n_fold, shuffle = True, random_state=RANDOM_SEED)\n",
    "        # create the target encoding\n",
    "        col_mean_name = self.feature + '_target'\n",
    "        X[col_mean_name] = np.nan\n",
    "\n",
    "        for train_index, val_index in kf.split(X):\n",
    "            X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "            X.loc[X.index[val_index], col_mean_name] = \\\n",
    "            X_val[self.feature].map(X_train.groupby(self.feature)[self.target].mean())\n",
    "        # missing value imputation\n",
    "        X[col_mean_name].fillna(mean_of_target, inplace = True)\n",
    "\n",
    "        if self.verbose:\n",
    "            encoded_feature = X[col_mean_name].values\n",
    "            print('Correlation between {} and, {} is {}.'.\\\n",
    "                  format(col_mean_name, self.target,\n",
    "                         np.corrcoef(X[self.target].values, encoded_feature)[0][1]))\n",
    "        # discard orginal feature column if needed\n",
    "        if self.discard_original:\n",
    "            X = X.drop(self.target, axis=1)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:56:27.166530Z",
     "iopub.status.busy": "2021-07-03T00:56:27.166225Z",
     "iopub.status.idle": "2021-07-03T00:56:27.180480Z",
     "shell.execute_reply": "2021-07-03T00:56:27.179528Z",
     "shell.execute_reply.started": "2021-07-03T00:56:27.166463Z"
    }
   },
   "outputs": [],
   "source": [
    "class KFoldTargetEncoderTest(base.BaseEstimator, base.TransformerMixin):\n",
    "    \"\"\"\n",
    "    This object contains a target encoder for a testing set which should have\n",
    "    both X and y.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    train:          pandas DataFrame. The training DataFrame with the feature and \n",
    "                    the target encoded column of it.\n",
    "    feature:        string. The column name of the feature.\n",
    "    feature_target: string. The column name of the feature_target that \n",
    "                    has been calculated in the training set.\n",
    "                    \n",
    "    Example\n",
    "    ---------\n",
    "    test_target_encoder = KFoldTargetEncoderTest(new_train, 'A', 'A_target')\n",
    "    new_test = test_target_encoder.transform(test)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, train, feature, feature_target):\n",
    "        \n",
    "        self.train = train\n",
    "        self.feature = feature\n",
    "        self.feature_target = feature_target\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,X):\n",
    "        \"\"\"\n",
    "        Transform the testing set based on K-fold target encoder of the training set.\n",
    "        Notice this function can only encode one feature at a time.\n",
    "        \n",
    "        Argument\n",
    "        --------\n",
    "        X: pandas DataFrame. The testing set to be transformed.\n",
    "        \n",
    "        Output\n",
    "        --------\n",
    "        X: A pandas DataFrame with transformed target encoding.\n",
    "        \"\"\"\n",
    "\n",
    "        mean = self.train[[self.feature,self.feature_target]].groupby(self.feature).mean().reset_index() \n",
    "        \n",
    "        dd = {}\n",
    "        for index, row in mean.iterrows():\n",
    "            dd[row[self.feature]] = row[self.feature_target]\n",
    "\n",
    "        \n",
    "        X[self.feature_target] = X[self.feature]\n",
    "        X = X.replace({self.feature_target: dd})\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:56:27.182145Z",
     "iopub.status.busy": "2021-07-03T00:56:27.181852Z",
     "iopub.status.idle": "2021-07-03T00:56:27.569362Z",
     "shell.execute_reply": "2021-07-03T00:56:27.568290Z",
     "shell.execute_reply.started": "2021-07-03T00:56:27.182115Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.concat([X_train_a, y_train], axis=1)\n",
    "new_train = train_df.copy()\n",
    "for feature in strong_predictors:\n",
    "    train_target_encoder = KFoldTargetEncoderTrain(feature, 'Y')\n",
    "    new_train = train_target_encoder.fit_transform(new_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the correlations between those categorical features and the target, the strong predictors (correlation $\\ge$ 0.1) are **destination, passanger, weather, time, coupon,** and **expiration.** Although not all of the features in the `strong_predictors` list are really strong predictors, we still want to implement the frequency encoding and feature encoding on them because it can greatly reduce the dimensions of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:56:27.570856Z",
     "iopub.status.busy": "2021-07-03T00:56:27.570584Z",
     "iopub.status.idle": "2021-07-03T00:56:27.666570Z",
     "shell.execute_reply": "2021-07-03T00:56:27.665747Z",
     "shell.execute_reply.started": "2021-07-03T00:56:27.570829Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df = pd.concat([X_test_a, y_test], axis=1)\n",
    "new_test = test_df.copy()\n",
    "strong_predictors_targets = []\n",
    "for feature in strong_predictors:\n",
    "    strong_predictors_targets.append(feature + '_target')\n",
    "for feature, feature_target in zip(strong_predictors, strong_predictors_targets):\n",
    "    test_target_encoder = KFoldTargetEncoderTest(new_train, feature, feature_target)\n",
    "    new_test = test_target_encoder.transform(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:56:27.667965Z",
     "iopub.status.busy": "2021-07-03T00:56:27.667709Z",
     "iopub.status.idle": "2021-07-03T00:56:27.676940Z",
     "shell.execute_reply": "2021-07-03T00:56:27.675867Z",
     "shell.execute_reply.started": "2021-07-03T00:56:27.667940Z"
    }
   },
   "outputs": [],
   "source": [
    "new_train.drop(columns=strong_predictors, inplace=True)\n",
    "new_test.drop(columns=strong_predictors, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:56:27.678097Z",
     "iopub.status.busy": "2021-07-03T00:56:27.677827Z",
     "iopub.status.idle": "2021-07-03T00:56:27.689784Z",
     "shell.execute_reply": "2021-07-03T00:56:27.688904Z",
     "shell.execute_reply.started": "2021-07-03T00:56:27.678065Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_a = new_train.drop(columns=['Y'])\n",
    "X_test_a = new_test.drop(columns=['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:56:27.691549Z",
     "iopub.status.busy": "2021-07-03T00:56:27.691261Z",
     "iopub.status.idle": "2021-07-03T00:56:27.715517Z",
     "shell.execute_reply": "2021-07-03T00:56:27.714210Z",
     "shell.execute_reply.started": "2021-07-03T00:56:27.691521Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_a.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other Data Preprocessing**\n",
    "\n",
    "Here we used sklearn's `Pipeline` and `ColumnTransformer` to do simple imputation, standardization, and one hot encoder for columns of mixed data types. The tutorial of how to use `Pipeline` and `ColumnTransformer` can be found [here](https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:56:27.717334Z",
     "iopub.status.busy": "2021-07-03T00:56:27.716932Z",
     "iopub.status.idle": "2021-07-03T00:56:27.782263Z",
     "shell.execute_reply": "2021-07-03T00:56:27.781403Z",
     "shell.execute_reply.started": "2021-07-03T00:56:27.717290Z"
    }
   },
   "outputs": [],
   "source": [
    "num_features_a = X_train_a.select_dtypes(['int64', 'float64']).columns\n",
    "cat_features_a = X_train_a.select_dtypes(['object']).columns\n",
    "num_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "cat_transformer = OneHotEncoder()\n",
    "preprocessor_a = ColumnTransformer(transformers=[\n",
    "    ('num', num_transformer, num_features_a),\n",
    "    ('cat', cat_transformer, cat_features_a)\n",
    "])\n",
    "X_train_a = preprocessor_a.fit_transform(X_train_a)\n",
    "X_test_a = preprocessor_a.transform(X_test_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:56:27.784140Z",
     "iopub.status.busy": "2021-07-03T00:56:27.783813Z",
     "iopub.status.idle": "2021-07-03T00:56:27.790559Z",
     "shell.execute_reply": "2021-07-03T00:56:27.789629Z",
     "shell.execute_reply.started": "2021-07-03T00:56:27.784108Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_a.shape, X_test_a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After data preprocessing, we now have 33 features in plan A. Now, we can do data preprocessing for plan B, which basically uses `OneHotEncoder` for all categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plan B**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:56:27.792312Z",
     "iopub.status.busy": "2021-07-03T00:56:27.791978Z",
     "iopub.status.idle": "2021-07-03T00:56:27.805514Z",
     "shell.execute_reply": "2021-07-03T00:56:27.803692Z",
     "shell.execute_reply.started": "2021-07-03T00:56:27.792281Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_b = X_train.copy()\n",
    "X_test_b = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:56:27.807499Z",
     "iopub.status.busy": "2021-07-03T00:56:27.807199Z",
     "iopub.status.idle": "2021-07-03T00:56:27.903012Z",
     "shell.execute_reply": "2021-07-03T00:56:27.902089Z",
     "shell.execute_reply.started": "2021-07-03T00:56:27.807469Z"
    }
   },
   "outputs": [],
   "source": [
    "num_features_b = X_train_b.select_dtypes(['int64', 'float64']).columns\n",
    "cat_features_b = X_train_b.select_dtypes(['object']).columns\n",
    "\n",
    "preprocessor_b = ColumnTransformer(transformers=[\n",
    "    ('num', num_transformer, num_features_b),\n",
    "    ('cat', cat_transformer, cat_features_b)\n",
    "])\n",
    "X_train_b = preprocessor_b.fit_transform(X_train_b)\n",
    "X_test_b = preprocessor_b.transform(X_test_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:56:27.904697Z",
     "iopub.status.busy": "2021-07-03T00:56:27.904407Z",
     "iopub.status.idle": "2021-07-03T00:56:27.910725Z",
     "shell.execute_reply": "2021-07-03T00:56:27.909820Z",
     "shell.execute_reply.started": "2021-07-03T00:56:27.904668Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_b.shape, X_test_b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have both kinds of data: plan A with frequency encoding and target encoding, plan B with one hot encoding. We will next build basic models on them and check the performance between plan A and B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic models we picked are Logistic Regression, Decision Tree, Naive Bayes, K Nearest Neighbor, and linear Support Vector Machine. We picked those models because they are fast to train. We will use `RandomizedSearchCV` to choose hyperparameters from the same parameter grid for both plan A and plan B. After that, we will compare the results of them.\n",
    "\n",
    "Tutorial for `RandomizedSearchCV` can be found [here](https://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html#sphx-glr-auto-examples-model-selection-plot-randomized-search-py). We also borrowed Rupanjan Nayak's [code](https://stackoverflow.com/questions/23045318/scikit-grid-search-over-multiple-classifiers) to train multiper models at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plan A**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:56:27.912579Z",
     "iopub.status.busy": "2021-07-03T00:56:27.912205Z",
     "iopub.status.idle": "2021-07-03T00:56:27.923716Z",
     "shell.execute_reply": "2021-07-03T00:56:27.922627Z",
     "shell.execute_reply.started": "2021-07-03T00:56:27.912550Z"
    }
   },
   "outputs": [],
   "source": [
    "logreg_clf = LogisticRegression(solver='saga', max_iter=500,\n",
    "                               random_state=RANDOM_SEED)\n",
    "dt_clf = DecisionTreeClassifier(random_state=RANDOM_SEED)\n",
    "bnb_clf = BernoulliNB()\n",
    "knn_clf = KNeighborsClassifier()\n",
    "lsvm_clf = LinearSVC(max_iter=5000, dual=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:56:27.926131Z",
     "iopub.status.busy": "2021-07-03T00:56:27.925716Z",
     "iopub.status.idle": "2021-07-03T00:56:27.938795Z",
     "shell.execute_reply": "2021-07-03T00:56:27.938011Z",
     "shell.execute_reply.started": "2021-07-03T00:56:27.926088Z"
    }
   },
   "outputs": [],
   "source": [
    "logreg_params = dict(C=loguniform(1e-1, 1e2),\n",
    "                     penalty=['l1', 'l2'])\n",
    "dt_params = dict(criterion=['gini', 'entropy'],\n",
    "                 min_samples_split=[2, 4, 6, 8, 10],\n",
    "                 max_depth=[2, 4, 6, 8, 10])\n",
    "bnb_params = dict(alpha=loguniform(1e-1, 1e0))\n",
    "knn_params = dict(n_neighbors=[2, 4, 6, 8, 10, 12, 14, 20],\n",
    "               weights=['uniform', 'distance'],\n",
    "               metric=['euclidean', 'manhattan'])\n",
    "lsvm_params = dict(C=loguniform(1e-1, 1e2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:56:27.941220Z",
     "iopub.status.busy": "2021-07-03T00:56:27.940353Z",
     "iopub.status.idle": "2021-07-03T00:56:27.956440Z",
     "shell.execute_reply": "2021-07-03T00:56:27.955656Z",
     "shell.execute_reply.started": "2021-07-03T00:56:27.941142Z"
    }
   },
   "outputs": [],
   "source": [
    "clf_list = [logreg_clf, dt_clf, bnb_clf, knn_clf, lsvm_clf]\n",
    "params_list = [logreg_params, dt_params, bnb_params, knn_params, lsvm_params]\n",
    "model_names = ['Logistic Regression', 'Decison Tree', 'Bernoulli Naive Bayes',\n",
    "               'KNN Classifier', 'Linear SVM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:56:27.957903Z",
     "iopub.status.busy": "2021-07-03T00:56:27.957624Z",
     "iopub.status.idle": "2021-07-03T00:56:27.967499Z",
     "shell.execute_reply": "2021-07-03T00:56:27.966348Z",
     "shell.execute_reply.started": "2021-07-03T00:56:27.957874Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(X, y, model_suffix, clf_list=clf_list, params_list=params_list):\n",
    "    for i in range(len(clf_list)):\n",
    "        # model training with RandomizedSearchCV\n",
    "        rscv = RandomizedSearchCV(estimator=clf_list[i],\n",
    "                                  param_distributions=params_list[i],\n",
    "                                  n_jobs=-1, random_state=RANDOM_SEED).fit(X, y)\n",
    "        # store cv results\n",
    "        globals()['rscv%s' % model_suffix[i]] = pd.DataFrame(rscv.cv_results_)\n",
    "        # store the best model\n",
    "        globals()['best%s' % model_suffix[i]] = rscv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:56:27.969179Z",
     "iopub.status.busy": "2021-07-03T00:56:27.968883Z",
     "iopub.status.idle": "2021-07-03T00:56:27.979424Z",
     "shell.execute_reply": "2021-07-03T00:56:27.978018Z",
     "shell.execute_reply.started": "2021-07-03T00:56:27.969150Z"
    }
   },
   "outputs": [],
   "source": [
    "def record_best_result(model_list, model_suffix):\n",
    "    # store the best results into a dataframe\n",
    "    for i in range(len(model_list)):\n",
    "        globals()['df%s' % model_suffix[i]] = model_list[i].query('rank_test_score == 1')\\\n",
    "        [['params', 'mean_test_score', 'std_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:56:27.981284Z",
     "iopub.status.busy": "2021-07-03T00:56:27.980832Z",
     "iopub.status.idle": "2021-07-03T00:56:27.991422Z",
     "shell.execute_reply": "2021-07-03T00:56:27.990313Z",
     "shell.execute_reply.started": "2021-07-03T00:56:27.981252Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_eval(clf_list, model_names, X_test, y_test):\n",
    "    test_acc = []\n",
    "    f1_score = []\n",
    "    for clf in clf_list:\n",
    "        test_acc.append(clf.score(X_test, y_test))\n",
    "        f1_score.append(metrics.f1_score(y_test, clf.predict(X_test)))\n",
    "    return pd.DataFrame(data={'model': model_names, 'test_acc': test_acc, 'f1_score': f1_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:56:27.993256Z",
     "iopub.status.busy": "2021-07-03T00:56:27.992895Z",
     "iopub.status.idle": "2021-07-03T00:57:05.455301Z",
     "shell.execute_reply": "2021-07-03T00:57:05.454254Z",
     "shell.execute_reply.started": "2021-07-03T00:56:27.993214Z"
    }
   },
   "outputs": [],
   "source": [
    "# train models\n",
    "model_suffix_a = ['_logreg_a', '_dt_a', '_bnb_a', '_knn_a', '_lsvm_a']\n",
    "train_model(X_train_a, y_train, model_suffix_a)\n",
    "\n",
    "# record best results in cross validation\n",
    "rscv_list_a = [rscv_logreg_a, rscv_dt_a, rscv_bnb_a, rscv_knn_a, rscv_lsvm_a]\n",
    "record_best_result(rscv_list_a, model_suffix_a)\n",
    "\n",
    "# output the best results as a dataframe\n",
    "df_list_a = [df_logreg_a, df_dt_a, df_bnb_a, df_knn_a, df_lsvm_a]\n",
    "for df, model in zip(df_list_a, model_names):\n",
    "    df['model'] = model\n",
    "result_df_a = pd.concat(df_list_a)\n",
    "\n",
    "# check test scores\n",
    "best_clfs_a = [best_logreg_a, best_dt_a, best_bnb_a, best_knn_a, best_lsvm_a]\n",
    "test_result_a = model_eval(best_clfs_a, model_names, X_test_a, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:57:05.457227Z",
     "iopub.status.busy": "2021-07-03T00:57:05.456826Z",
     "iopub.status.idle": "2021-07-03T00:57:05.476880Z",
     "shell.execute_reply": "2021-07-03T00:57:05.475810Z",
     "shell.execute_reply.started": "2021-07-03T00:57:05.457182Z"
    }
   },
   "outputs": [],
   "source": [
    "result_df_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:57:05.479127Z",
     "iopub.status.busy": "2021-07-03T00:57:05.478477Z",
     "iopub.status.idle": "2021-07-03T00:57:05.495414Z",
     "shell.execute_reply": "2021-07-03T00:57:05.494365Z",
     "shell.execute_reply.started": "2021-07-03T00:57:05.479068Z"
    }
   },
   "outputs": [],
   "source": [
    "test_result_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plan B**\n",
    "\n",
    "Train those models again on data of plan B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:57:05.497732Z",
     "iopub.status.busy": "2021-07-03T00:57:05.497078Z",
     "iopub.status.idle": "2021-07-03T00:57:59.381386Z",
     "shell.execute_reply": "2021-07-03T00:57:59.380493Z",
     "shell.execute_reply.started": "2021-07-03T00:57:05.497679Z"
    }
   },
   "outputs": [],
   "source": [
    "# train models\n",
    "model_suffix_b = ['_logreg_b', '_dt_b', '_bnb_b', '_knn_b', '_lsvm_b']\n",
    "train_model(X_train_b, y_train, model_suffix_b)\n",
    "\n",
    "# record best results in cross validation\n",
    "rscv_list_b = [rscv_logreg_b, rscv_dt_b, rscv_bnb_b, rscv_knn_b, rscv_lsvm_b]\n",
    "record_best_result(rscv_list_b, model_suffix_b)\n",
    "\n",
    "# output the best results as a dataframe\n",
    "df_list_b = [df_logreg_b, df_dt_b, df_bnb_b, df_knn_b, df_lsvm_b]\n",
    "for df, model in zip(df_list_b, model_names):\n",
    "    df['model'] = model\n",
    "result_df_b = pd.concat(df_list_b)\n",
    "\n",
    "# check test scores\n",
    "best_clfs_b = [best_logreg_b, best_dt_b, best_bnb_b, best_knn_b, best_lsvm_b]\n",
    "test_result_b = model_eval(best_clfs_b, model_names, X_test_b, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:57:59.382993Z",
     "iopub.status.busy": "2021-07-03T00:57:59.382703Z",
     "iopub.status.idle": "2021-07-03T00:57:59.398187Z",
     "shell.execute_reply": "2021-07-03T00:57:59.397039Z",
     "shell.execute_reply.started": "2021-07-03T00:57:59.382963Z"
    }
   },
   "outputs": [],
   "source": [
    "result_df_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:57:59.401544Z",
     "iopub.status.busy": "2021-07-03T00:57:59.401254Z",
     "iopub.status.idle": "2021-07-03T00:57:59.415338Z",
     "shell.execute_reply": "2021-07-03T00:57:59.414279Z",
     "shell.execute_reply.started": "2021-07-03T00:57:59.401517Z"
    }
   },
   "outputs": [],
   "source": [
    "test_result_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:57:59.417161Z",
     "iopub.status.busy": "2021-07-03T00:57:59.416692Z",
     "iopub.status.idle": "2021-07-03T00:57:59.549145Z",
     "shell.execute_reply": "2021-07-03T00:57:59.548280Z",
     "shell.execute_reply.started": "2021-07-03T00:57:59.417117Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=test_result_a.model, y=test_result_a.test_acc,\n",
    "                         mode='lines+markers', name='testing accuracy of plan A'))\n",
    "fig.add_trace(go.Scatter(x=test_result_b.model, y=test_result_b.test_acc,\n",
    "                         mode='lines+markers', name='testing accuracy of plan B'))\n",
    "fig.update_layout(title={'text': 'Testing Accuracy of Basic Models',\n",
    "                         'y': 0.9,\n",
    "                         'x': 0.4,\n",
    "                         'xanchor': 'center',\n",
    "                         'yanchor': 'top'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:57:59.550843Z",
     "iopub.status.busy": "2021-07-03T00:57:59.550549Z",
     "iopub.status.idle": "2021-07-03T00:57:59.566692Z",
     "shell.execute_reply": "2021-07-03T00:57:59.565453Z",
     "shell.execute_reply.started": "2021-07-03T00:57:59.550813Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=test_result_a.model, y=test_result_a.f1_score,\n",
    "                         mode='lines+markers', name='testing f1 score of plan A'))\n",
    "fig.add_trace(go.Scatter(x=test_result_b.model, y=test_result_b.f1_score,\n",
    "                         mode='lines+markers', name='testing f1 score of plan B'))\n",
    "fig.update_layout(title={'text': 'Testing F1 Scores of Basic Models',\n",
    "                         'y': 0.9,\n",
    "                         'x': 0.4,\n",
    "                         'xanchor': 'center',\n",
    "                         'yanchor': 'top'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the visualizations, we can see that the knn classifier built on data of plan A has a better accuracy. This is quite interpretable since knn classifier relies on distances, and our feature engineering in plan A transformed sparse categorical data into numerical frequency features, by doing that, distance among the feature space started to make more sense. \n",
    "\n",
    "Decision tree built on data of plan B performs better than plan A, that is because decision tree is good at splitting categorical data, so one hot encoded features are more suitable for decision tree. \n",
    "\n",
    "As for F1 scores, knn classifier reached the highest testing F1 score on data of plan A, indicating the best overall predictive accuracy behind it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the prediction results, we have two directions to explore: \n",
    "* Feature Expansion\n",
    "\n",
    "We can use clustering method to cluster the training data and add the cluster label as new feature.\n",
    "\n",
    "* Advanced Models\n",
    "\n",
    "Advanced models like Random Forest, kernel SVM, Neural Networks, and Gradient Boosting Machine can be applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two popular clustering methods can be used to apply feature expansion: hierarchical clustering and k-means clustering. However, hierarchical clustering is slower than k-means clustering, thus, we are using k-means for this feature expansion task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-prototypes Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have mixed data types, K-means is not appropriate here because Euclidean distance is meaningless for categorical data. There are two variations of K-means clustering: K-modes is capable of clustering on categorical data, K-prototypes clustering is suitable for mixed data types. Both of them are encapsulated in [**kmodes**](https://github.com/nicodv/kmodes) module. The original paper of K-prototypes clustering by Huang can be found [here](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.94.9984), another version of k-prototypes by Cao et al. can be found [here](http://www.jiyeliang.net/Cms_Data/Contents/SXU_JYL/Folders/JournalPapers/~contents/TW82GXVHZCGNRBPV/A%20new%20initialization%20method%20for%20categorical%20data%20clustering.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a concern for the clustering algorithm: to include the target feature or not. If we include the target to get the cluster labels, and use those labels to classify the target, the information about target will have leaked into the features, and the accuracy will be overly optimisitc. But when we evaluate them on the test set, this bias will diminish and disappear. Thus, we can also use k-fold cross validation to alleviate the data leakage problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use k-prototypes, we need to point out the column indices of categorical features. After using sklearn's preprocessor, the column names and indices have changed. So, we need to figure them out for data plan A and B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:57:59.568358Z",
     "iopub.status.busy": "2021-07-03T00:57:59.568080Z",
     "iopub.status.idle": "2021-07-03T00:57:59.581248Z",
     "shell.execute_reply": "2021-07-03T00:57:59.580168Z",
     "shell.execute_reply.started": "2021-07-03T00:57:59.568329Z"
    }
   },
   "outputs": [],
   "source": [
    "scale_features_a = list(preprocessor_a.transformers_[0][2])\n",
    "onehot_features_a = list(preprocessor_a.transformers_[1][1].get_feature_names(list(cat_features_a)))\n",
    "onehot_indices_a = list(range(len(preprocessor_a.transformers_[0][2]), \n",
    "                                  X_train_a.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:57:59.583340Z",
     "iopub.status.busy": "2021-07-03T00:57:59.582867Z",
     "iopub.status.idle": "2021-07-03T00:57:59.594454Z",
     "shell.execute_reply": "2021-07-03T00:57:59.593509Z",
     "shell.execute_reply.started": "2021-07-03T00:57:59.583306Z"
    }
   },
   "outputs": [],
   "source": [
    "scale_features_b = list(preprocessor_b.transformers_[0][2])\n",
    "onehot_features_b = list(preprocessor_b.transformers_[1][1].get_feature_names(list(cat_features_b)))\n",
    "onehot_indices_b = list(range(len(preprocessor_b.transformers_[0][2]), \n",
    "                                  X_train_b.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine which number of clusters to use in the K-prototypes clustering, we plotted a silhouette diagram:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plan A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:57:59.596303Z",
     "iopub.status.busy": "2021-07-03T00:57:59.595971Z",
     "iopub.status.idle": "2021-07-03T00:57:59.608114Z",
     "shell.execute_reply": "2021-07-03T00:57:59.606042Z",
     "shell.execute_reply.started": "2021-07-03T00:57:59.596271Z"
    }
   },
   "outputs": [],
   "source": [
    "train_kprot_a = np.hstack((X_train_a, y_train[:, np.newaxis]*5.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T00:57:59.609913Z",
     "iopub.status.busy": "2021-07-03T00:57:59.609609Z",
     "iopub.status.idle": "2021-07-03T01:08:33.300019Z",
     "shell.execute_reply": "2021-07-03T01:08:33.299015Z",
     "shell.execute_reply.started": "2021-07-03T00:57:59.609882Z"
    }
   },
   "outputs": [],
   "source": [
    "kprot_runs_a = [KPrototypes(n_clusters=k, random_state=RANDOM_SEED, n_jobs=-1).\\\n",
    "                fit(train_kprot_a, categorical=onehot_indices_a)\n",
    "                for k in range(1, 11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T01:08:33.301856Z",
     "iopub.status.busy": "2021-07-03T01:08:33.301555Z",
     "iopub.status.idle": "2021-07-03T01:08:33.312041Z",
     "shell.execute_reply": "2021-07-03T01:08:33.310964Z",
     "shell.execute_reply.started": "2021-07-03T01:08:33.301828Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_silhouette(X_train, kprot_runs):\n",
    "    silhouette_scores = [silhouette_score(X_train, model.labels_) \n",
    "                         for model in kprot_runs[1:]]\n",
    "    \n",
    "    plt.figure(figsize=(15, 20))\n",
    "    for k in range(2, 10):\n",
    "        plt.subplot(4, 2, k - 1)\n",
    "\n",
    "        y_pred = kprot_runs[k - 1].labels_\n",
    "        silhouette_coefficients = silhouette_samples(X_train, y_pred)\n",
    "\n",
    "        padding = len(X_train) // 30\n",
    "        pos = padding\n",
    "        ticks = []\n",
    "        for i in range(k):\n",
    "            coeffs = silhouette_coefficients[y_pred == i]\n",
    "            coeffs.sort()\n",
    "\n",
    "            color = cm.Spectral(i / k)\n",
    "            plt.fill_betweenx(np.arange(pos, pos + len(coeffs)), 0, coeffs,\n",
    "                              facecolor=color, edgecolor=color, alpha=0.7)\n",
    "            ticks.append(pos + len(coeffs) // 2)\n",
    "            pos += len(coeffs) + padding\n",
    "\n",
    "        plt.gca().yaxis.set_major_locator(FixedLocator(ticks))\n",
    "        plt.gca().yaxis.set_major_formatter(FixedFormatter(range(k)))\n",
    "        if k in (2, 4, 6, 8):\n",
    "            plt.ylabel(\"Cluster\")\n",
    "\n",
    "        if k in (8, 9):\n",
    "            plt.xlabel(\"Silhouette Coefficient\")\n",
    "\n",
    "        plt.axvline(x=silhouette_scores[k - 1], color=\"red\", linestyle=\"--\")\n",
    "        plt.title(\"$k={}$\".format(k), fontsize=16)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T01:08:33.314237Z",
     "iopub.status.busy": "2021-07-03T01:08:33.313801Z",
     "iopub.status.idle": "2021-07-03T01:09:08.667041Z",
     "shell.execute_reply": "2021-07-03T01:09:08.666271Z",
     "shell.execute_reply.started": "2021-07-03T01:08:33.314195Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_silhouette(train_kprot_a, kprot_runs_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette coefficient ranges from -1 to +1, the silhouette score is the mean silhouette coefficient over all the instances within the cluster. A coefficient close to +1 means that the instance is well inside its own cluster and far from other clusters, while a coefficient close to 0 means that it is close to a cluster boundary, and finally a coefficient close to 1 means that the instance may have been assigned to the wrong cluster.\n",
    "\n",
    "According to the silhouette plot for data plan A, the silhouette score looks good when $k = 2, 3, 4, 5$ as the red dashed line indicates the average silhouette score. We are picking $k = 4$ as the number of clusters of our k-prototypes clustering for data in plan A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plan B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to sparsity and the curse of dimension, k-prototypes model on data of plan B will take more than 1 hour to train for plotting the silhouette plot. Thus, we give it up. The feature expansion will only be applied on data in plan A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to implement the k-prototypes algorithm for feature expansion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T01:09:08.668405Z",
     "iopub.status.busy": "2021-07-03T01:09:08.668036Z",
     "iopub.status.idle": "2021-07-03T01:09:08.678511Z",
     "shell.execute_reply": "2021-07-03T01:09:08.677628Z",
     "shell.execute_reply.started": "2021-07-03T01:09:08.668377Z"
    }
   },
   "outputs": [],
   "source": [
    "class KProtFeaturizer:\n",
    "    \"\"\"\n",
    "    Transforms mixed data into k-prototypes cluster.\n",
    "    \n",
    "    This transformer runs k-prototypes on the input data and converts each data\n",
    "    point into the ID of the closest cluster. If a target variable is present, \n",
    "    it is scaled and included as input to k-prototypes in order to derive clusters\n",
    "    that obey the classification boundary as well as group similar points together.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k=15, target_scale=5.0, \n",
    "                 categorical=None,\n",
    "                 random_state=RANDOM_SEED):\n",
    "        self.k = k\n",
    "        self.target_scale = target_scale\n",
    "        self.categorical = categorical\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Runs k-prototypes on the input data and finds centroids.\"\"\"\n",
    "        \n",
    "        if y is None:\n",
    "            # No target variable, just do plain k-prototypes\n",
    "            kprot_model = KPrototypes(n_clusters=self.k,\n",
    "                                      n_jobs=-1,\n",
    "                                      random_state=self.random_state)\n",
    "            kprot_model.fit(X, categorical=self.categorical)\n",
    "            \n",
    "            self.kprot_model = kprot_model\n",
    "            self.cluster_centroids_ = kprot_model.cluster_centroids_\n",
    "            return self\n",
    "        \n",
    "        # There is target information. Apply approrpriate scaling and include\n",
    "        # it in the input data to k-prototypes.\n",
    "        data_with_target = np.hstack((X, y[:, np.newaxis] * self.target_scale))\n",
    "        \n",
    "        # Build a pre-training k-prototypes model on data and target\n",
    "        kprot_model_pretrain = KPrototypes(n_clusters=self.k,\n",
    "                                           n_jobs=-1,\n",
    "                                           random_state=self.random_state)\n",
    "        kprot_model_pretrain.fit(data_with_target, categorical=self.categorical)\n",
    "        \n",
    "        # Run k-prototypes a second time to get the clusters in the original space\n",
    "        # without target info. Initialize using centroids found in pre-training. \n",
    "        # Go through a single iteration of cluster assignment and centroid recomputation.\n",
    "        kprot_model = KPrototypes(n_clusters=self.k,\n",
    "                                  # For KPrototypes, we need to specify the cluster centroids for \n",
    "                                  # numerical and categorical columns, notice that the numerical \n",
    "                                  # part should exclude the target info\n",
    "                                  init=[kprot_model_pretrain.cluster_centroids_[:, \\\n",
    "                                  [i for i in range(kprot_model_pretrain.cluster_centroids_.shape[1] - 1)\\\n",
    "                                  if i not in self.categorical]],\\\n",
    "                                  kprot_model_pretrain.cluster_centroids_[:, self.categorical]],\n",
    "                                  n_init=1,\n",
    "                                  max_iter=1)\n",
    "        kprot_model.fit(X, categorical=self.categorical)\n",
    "        \n",
    "        self.kprot_model = kprot_model\n",
    "        self.cluster_centroids_ = kprot_model.cluster_centroids_\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"Outputs the closest cluster ID for each input data point.\n",
    "        \"\"\"\n",
    "        clusters = self.kprot_model.predict(X, categorical=self.categorical)\n",
    "        return clusters[:, np.newaxis]\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T01:09:08.679902Z",
     "iopub.status.busy": "2021-07-03T01:09:08.679526Z",
     "iopub.status.idle": "2021-07-03T01:10:01.166142Z",
     "shell.execute_reply": "2021-07-03T01:10:01.165099Z",
     "shell.execute_reply.started": "2021-07-03T01:09:08.679875Z"
    }
   },
   "outputs": [],
   "source": [
    "# apply the featurizer\n",
    "kprot_a = KProtFeaturizer(k=4, target_scale=5.0, \n",
    "                          categorical=onehot_indices_a,\n",
    "                          random_state=RANDOM_SEED)\n",
    "train_cluster_a = kprot_a.fit_transform(X_train_a, y_train)\n",
    "test_cluster_a = kprot_a.transform(X_test_a)\n",
    "# use one-hot encoder to record the cluster labels\n",
    "enc = OneHotEncoder()\n",
    "# Notice that after one-hot encoding, the outcome matrix\n",
    "# is a sparse matrix, we need to transform to an ndarray\n",
    "train_cluster_label_a = enc.fit_transform(train_cluster_a).toarray()\n",
    "test_cluster_label_a = enc.transform(test_cluster_a).toarray()\n",
    "X_train_kprot_a = np.hstack((X_train_a, train_cluster_label_a))\n",
    "X_test_kprot_a = np.hstack((X_test_a, test_cluster_label_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain Models after Adding Cluster Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T01:10:01.168196Z",
     "iopub.status.busy": "2021-07-03T01:10:01.167769Z",
     "iopub.status.idle": "2021-07-03T01:10:42.025816Z",
     "shell.execute_reply": "2021-07-03T01:10:42.024284Z",
     "shell.execute_reply.started": "2021-07-03T01:10:01.168153Z"
    }
   },
   "outputs": [],
   "source": [
    "# train models\n",
    "model_suffix_kprot = ['_logreg_kprot', '_dt_kprot', '_bnb_kprot', '_knn_kprot', '_lsvm_kprot']\n",
    "train_model(X_train_kprot_a, y_train, model_suffix_kprot)\n",
    "\n",
    "# record best results in cross validation\n",
    "rscv_list_kprot = [rscv_logreg_kprot, rscv_dt_kprot, rscv_bnb_kprot, rscv_knn_kprot, rscv_lsvm_kprot]\n",
    "record_best_result(rscv_list_kprot, model_suffix_kprot)\n",
    "\n",
    "# output the best results as a dataframe\n",
    "df_list_kprot = [df_logreg_kprot, df_dt_kprot, df_bnb_kprot, df_knn_kprot, df_lsvm_kprot]\n",
    "for df, model in zip(df_list_kprot, model_names):\n",
    "    df['model'] = model\n",
    "result_df_kprot = pd.concat(df_list_kprot)\n",
    "\n",
    "# check test scores\n",
    "best_clfs_kprot = [best_logreg_kprot, best_dt_kprot, best_bnb_kprot, best_knn_kprot, best_lsvm_kprot]\n",
    "test_result_kprot = model_eval(best_clfs_kprot, model_names, X_test_kprot_a, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T01:10:42.028310Z",
     "iopub.status.busy": "2021-07-03T01:10:42.027802Z",
     "iopub.status.idle": "2021-07-03T01:10:42.052776Z",
     "shell.execute_reply": "2021-07-03T01:10:42.051363Z",
     "shell.execute_reply.started": "2021-07-03T01:10:42.028264Z"
    }
   },
   "outputs": [],
   "source": [
    "result_df_kprot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T01:10:42.055006Z",
     "iopub.status.busy": "2021-07-03T01:10:42.054539Z",
     "iopub.status.idle": "2021-07-03T01:10:42.077409Z",
     "shell.execute_reply": "2021-07-03T01:10:42.075814Z",
     "shell.execute_reply.started": "2021-07-03T01:10:42.054961Z"
    }
   },
   "outputs": [],
   "source": [
    "test_result_kprot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T01:10:42.079769Z",
     "iopub.status.busy": "2021-07-03T01:10:42.079264Z",
     "iopub.status.idle": "2021-07-03T01:10:44.135408Z",
     "shell.execute_reply": "2021-07-03T01:10:44.134644Z",
     "shell.execute_reply.started": "2021-07-03T01:10:42.079722Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=test_result_a.model, y=test_result_a.test_acc,\n",
    "                         mode='lines+markers', name='plan A'))\n",
    "fig.add_trace(go.Scatter(x=test_result_b.model, y=test_result_b.test_acc,\n",
    "                         mode='lines+markers', name='plan B'))\n",
    "fig.add_trace(go.Scatter(x=test_result_kprot.model, y=test_result_kprot.test_acc,\n",
    "                         mode='lines+markers', name='after feature expansion'))\n",
    "fig.update_layout(title={'text': 'Testing Accuracy of Basic Models',\n",
    "                         'y': 0.9,\n",
    "                         'x': 0.4,\n",
    "                         'xanchor': 'center',\n",
    "                         'yanchor': 'top'},\n",
    "                  autosize=False,\n",
    "                  width=1000,\n",
    "                  height=600)\n",
    "fig.show()\n",
    "report = dp.Report(dp.Plot(fig))\n",
    "report.publish(name=\"Feature Engineering Methods Comparison\", open=True, visibility=dp.Visibility.PUBLIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T01:10:44.136975Z",
     "iopub.status.busy": "2021-07-03T01:10:44.136554Z",
     "iopub.status.idle": "2021-07-03T01:10:44.152775Z",
     "shell.execute_reply": "2021-07-03T01:10:44.151443Z",
     "shell.execute_reply.started": "2021-07-03T01:10:44.136945Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=test_result_a.model, y=test_result_a.f1_score,\n",
    "                         mode='lines+markers', name='testing f1 score of plan A'))\n",
    "fig.add_trace(go.Scatter(x=test_result_b.model, y=test_result_b.f1_score,\n",
    "                         mode='lines+markers', name='testing f1 score of plan B'))\n",
    "fig.add_trace(go.Scatter(x=test_result_kprot.model, y=test_result_kprot.f1_score,\n",
    "                         mode='lines+markers', name='testing accuracy after feature expansion'))\n",
    "fig.update_layout(title={'text': 'Testing F1 Scores of Basic Models',\n",
    "                         'y': 0.9,\n",
    "                         'x': 0.4,\n",
    "                         'xanchor': 'center',\n",
    "                         'yanchor': 'top'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the two plots above, feature expansion using kprototypes clustering decreased the performance of knn model, increased the performance of decision tree induction model. Both the highest accuracy and f1 scores were achieved by KNN classifier using data in plan A. Since we used the same random search grids, better results could have been achieved using different hyperparameters. Anyway, it is already enough to see the power of feature expansion using k-prototypes clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to apply on all three types of data kernel SVM and random forest. Neural networks and gradient boosting machines will be implemented in another notebook because they have good GPU support, but if I use GPU accelerator, the CPU power assigned will be lower, since sklearn mostly relys on CPU, we will seperate sklearn models from GPU supported models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we just call the feature expanded version of data plan C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T01:10:44.154796Z",
     "iopub.status.busy": "2021-07-03T01:10:44.154392Z",
     "iopub.status.idle": "2021-07-03T01:10:44.158745Z",
     "shell.execute_reply": "2021-07-03T01:10:44.158100Z",
     "shell.execute_reply.started": "2021-07-03T01:10:44.154753Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_c = X_train_kprot_a\n",
    "X_test_c = X_test_kprot_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, since our data is kinda huge, we will limit the search space of hyperparameters for advanced models as well. The next notebook will be talking about how to achieve best accuracy using model stacking and automl tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose three models: polynomial SVM, Gaussian RBF SVM, and Random Forest. Kernel tricks in SVM make it possible to get the same result as if we had added many non-linear features like by polynomial or Gaussian RBF, without actually having to add them. However, with our training data, the running time will be quite long. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T01:10:44.160422Z",
     "iopub.status.busy": "2021-07-03T01:10:44.159842Z",
     "iopub.status.idle": "2021-07-03T01:10:44.172552Z",
     "shell.execute_reply": "2021-07-03T01:10:44.171741Z",
     "shell.execute_reply.started": "2021-07-03T01:10:44.160391Z"
    }
   },
   "outputs": [],
   "source": [
    "poly_svc_clf = SVC(kernel='poly')\n",
    "rbf_svc_clf = SVC(kernel='rbf')\n",
    "# for gridsearchcv, n_jobs=-1 should be placed inside the classifier\n",
    "rf_clf = RandomForestClassifier(random_state=RANDOM_SEED, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T01:10:44.174272Z",
     "iopub.status.busy": "2021-07-03T01:10:44.173709Z",
     "iopub.status.idle": "2021-07-03T01:10:44.185377Z",
     "shell.execute_reply": "2021-07-03T01:10:44.184493Z",
     "shell.execute_reply.started": "2021-07-03T01:10:44.174230Z"
    }
   },
   "outputs": [],
   "source": [
    "poly_svc_params = dict(C=[0.1, 1, 10],\n",
    "                       degree=[2, 4, 10])\n",
    "rbf_svc_params = dict(C=[0.1, 1, 10],\n",
    "                   gamma=[0.1, 1, 2])\n",
    "rf_params = dict(n_estimators=[1000, 1400],\n",
    "                 max_depth=[20, 30],\n",
    "                 min_samples_split=[2, 5],\n",
    "                 min_samples_leaf=[1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T01:10:44.186858Z",
     "iopub.status.busy": "2021-07-03T01:10:44.186435Z",
     "iopub.status.idle": "2021-07-03T01:10:44.201282Z",
     "shell.execute_reply": "2021-07-03T01:10:44.200341Z",
     "shell.execute_reply.started": "2021-07-03T01:10:44.186823Z"
    }
   },
   "outputs": [],
   "source": [
    "ad_clf_list = [poly_svc_clf, rbf_svc_clf, rf_clf]\n",
    "ad_params_list = [poly_svc_params, rbf_svc_params, rf_params]\n",
    "ad_model_names = ['Polynomial SVM', 'Gaussian RBF SVM', 'Random Forest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T01:10:44.202774Z",
     "iopub.status.busy": "2021-07-03T01:10:44.202360Z",
     "iopub.status.idle": "2021-07-03T01:10:44.214361Z",
     "shell.execute_reply": "2021-07-03T01:10:44.213460Z",
     "shell.execute_reply.started": "2021-07-03T01:10:44.202742Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_ad_model(X, y, model_suffix, clf_list, params_list):\n",
    "    for i in range(len(clf_list)):\n",
    "        gscv = GridSearchCV(estimator=clf_list[i],\n",
    "                            param_grid=params_list[i]).fit(X, y)\n",
    "        globals()['gscv%s' % model_suffix[i]] = pd.DataFrame(gscv.cv_results_)\n",
    "        globals()['best%s' % model_suffix[i]] = gscv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* plan A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T01:10:44.215925Z",
     "iopub.status.busy": "2021-07-03T01:10:44.215446Z",
     "iopub.status.idle": "2021-07-03T01:30:17.402087Z",
     "shell.execute_reply": "2021-07-03T01:30:17.401107Z",
     "shell.execute_reply.started": "2021-07-03T01:10:44.215890Z"
    }
   },
   "outputs": [],
   "source": [
    "ad_model_suffix_a = ['_poly_svc_a', '_rbf_svc_a', '_rf_a']\n",
    "train_ad_model(X_train_a, y_train, ad_model_suffix_a, \n",
    "               clf_list=ad_clf_list, params_list=ad_params_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T01:30:17.403882Z",
     "iopub.status.busy": "2021-07-03T01:30:17.403511Z",
     "iopub.status.idle": "2021-07-03T01:30:17.424839Z",
     "shell.execute_reply": "2021-07-03T01:30:17.423361Z",
     "shell.execute_reply.started": "2021-07-03T01:30:17.403843Z"
    }
   },
   "outputs": [],
   "source": [
    "gscv_list_a = [gscv_poly_svc_a, gscv_rbf_svc_a, gscv_rf_a]\n",
    "record_best_result(gscv_list_a, ad_model_suffix_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T01:30:17.427207Z",
     "iopub.status.busy": "2021-07-03T01:30:17.426641Z",
     "iopub.status.idle": "2021-07-03T01:30:17.444093Z",
     "shell.execute_reply": "2021-07-03T01:30:17.443048Z",
     "shell.execute_reply.started": "2021-07-03T01:30:17.427160Z"
    }
   },
   "outputs": [],
   "source": [
    "df_list_ad_a = [df_poly_svc_a, df_rbf_svc_a, df_rf_a]\n",
    "for df, model in zip(df_list_ad_a, ad_model_names):\n",
    "    df['model'] = model\n",
    "result_df_ad_a = pd.concat(df_list_ad_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T01:30:17.445769Z",
     "iopub.status.busy": "2021-07-03T01:30:17.445469Z",
     "iopub.status.idle": "2021-07-03T01:30:23.381285Z",
     "shell.execute_reply": "2021-07-03T01:30:23.380235Z",
     "shell.execute_reply.started": "2021-07-03T01:30:17.445739Z"
    }
   },
   "outputs": [],
   "source": [
    "best_clfs_ad_a = [best_poly_svc_a, best_rbf_svc_a, best_rf_a]\n",
    "test_result_ad_a = model_eval(best_clfs_ad_a, ad_model_names, X_test_a, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T01:30:23.383142Z",
     "iopub.status.busy": "2021-07-03T01:30:23.382823Z",
     "iopub.status.idle": "2021-07-03T01:30:23.395767Z",
     "shell.execute_reply": "2021-07-03T01:30:23.394683Z",
     "shell.execute_reply.started": "2021-07-03T01:30:23.383106Z"
    }
   },
   "outputs": [],
   "source": [
    "result_df_ad_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T01:30:23.397246Z",
     "iopub.status.busy": "2021-07-03T01:30:23.396949Z",
     "iopub.status.idle": "2021-07-03T01:30:23.420963Z",
     "shell.execute_reply": "2021-07-03T01:30:23.420183Z",
     "shell.execute_reply.started": "2021-07-03T01:30:23.397218Z"
    }
   },
   "outputs": [],
   "source": [
    "test_result_ad_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* plan B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T01:30:23.422572Z",
     "iopub.status.busy": "2021-07-03T01:30:23.422166Z",
     "iopub.status.idle": "2021-07-03T02:30:43.350765Z",
     "shell.execute_reply": "2021-07-03T02:30:43.349614Z",
     "shell.execute_reply.started": "2021-07-03T01:30:23.422540Z"
    }
   },
   "outputs": [],
   "source": [
    "ad_model_suffix_b = ['_poly_svc_b', '_rbf_svc_b', '_rf_b']\n",
    "train_ad_model(X_train_b, y_train, ad_model_suffix_b, \n",
    "               clf_list=ad_clf_list, params_list=ad_params_list)\n",
    "gscv_list_b = [gscv_poly_svc_b, gscv_rbf_svc_b, gscv_rf_b]\n",
    "record_best_result(gscv_list_b, ad_model_suffix_b)\n",
    "df_list_ad_b = [df_poly_svc_b, df_rbf_svc_b, df_rf_b]\n",
    "for df, model in zip(df_list_ad_b, ad_model_names):\n",
    "    df['model'] = model\n",
    "result_df_ad_b = pd.concat(df_list_ad_b)\n",
    "best_clfs_ad_b = [best_poly_svc_b, best_rbf_svc_b, best_rf_b]\n",
    "test_result_ad_b = model_eval(best_clfs_ad_b, ad_model_names, X_test_b, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* plan C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T02:30:43.352448Z",
     "iopub.status.busy": "2021-07-03T02:30:43.352163Z",
     "iopub.status.idle": "2021-07-03T02:50:54.969197Z",
     "shell.execute_reply": "2021-07-03T02:50:54.968094Z",
     "shell.execute_reply.started": "2021-07-03T02:30:43.352419Z"
    }
   },
   "outputs": [],
   "source": [
    "ad_model_suffix_c = ['_poly_svc_c', '_rbf_svc_c', '_rf_c']\n",
    "train_ad_model(X_train_c, y_train, ad_model_suffix_c, \n",
    "               clf_list=ad_clf_list, params_list=ad_params_list)\n",
    "gscv_list_c = [gscv_poly_svc_c, gscv_rbf_svc_c, gscv_rf_c]\n",
    "record_best_result(gscv_list_c, ad_model_suffix_c)\n",
    "df_list_ad_c = [df_poly_svc_c, df_rbf_svc_c, df_rf_c]\n",
    "for df, model in zip(df_list_ad_c, ad_model_names):\n",
    "    df['model'] = model\n",
    "result_df_ad_c = pd.concat(df_list_ad_c)\n",
    "best_clfs_ad_c = [best_poly_svc_c, best_rbf_svc_c, best_rf_c]\n",
    "test_result_ad_c = model_eval(best_clfs_ad_c, ad_model_names, X_test_c, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T02:50:54.970873Z",
     "iopub.status.busy": "2021-07-03T02:50:54.970608Z",
     "iopub.status.idle": "2021-07-03T02:50:54.988102Z",
     "shell.execute_reply": "2021-07-03T02:50:54.987149Z",
     "shell.execute_reply.started": "2021-07-03T02:50:54.970847Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=test_result_ad_a.model, y=test_result_ad_a.test_acc,\n",
    "                         mode='lines+markers', name='testing accuracy of plan A'))\n",
    "fig.add_trace(go.Scatter(x=test_result_ad_b.model, y=test_result_ad_b.test_acc,\n",
    "                         mode='lines+markers', name='testing accuracy of plan B'))\n",
    "fig.add_trace(go.Scatter(x=test_result_ad_c.model, y=test_result_ad_c.test_acc,\n",
    "                         mode='lines+markers', name='testing accuracy after feature expansion'))\n",
    "fig.update_layout(title={'text': 'Testing Accuracy of Basic Models',\n",
    "                         'y': 0.9,\n",
    "                         'x': 0.4,\n",
    "                         'xanchor': 'center',\n",
    "                         'yanchor': 'top'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T02:50:54.989989Z",
     "iopub.status.busy": "2021-07-03T02:50:54.989618Z",
     "iopub.status.idle": "2021-07-03T02:50:55.041525Z",
     "shell.execute_reply": "2021-07-03T02:50:55.040730Z",
     "shell.execute_reply.started": "2021-07-03T02:50:54.989950Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=test_result_ad_a.model, y=test_result_ad_a.f1_score,\n",
    "                         mode='lines+markers', name='testing f1 score of plan A'))\n",
    "fig.add_trace(go.Scatter(x=test_result_ad_b.model, y=test_result_ad_b.f1_score,\n",
    "                         mode='lines+markers', name='testing f1 score of plan B'))\n",
    "fig.add_trace(go.Scatter(x=test_result_ad_c.model, y=test_result_ad_c.f1_score,\n",
    "                         mode='lines+markers', name='testing accuracy after feature expansion'))\n",
    "fig.update_layout(title={'text': 'Testing F1 Scores of Basic Models',\n",
    "                         'y': 0.9,\n",
    "                         'x': 0.4,\n",
    "                         'xanchor': 'center',\n",
    "                         'yanchor': 'top'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the plots, data plan B achieved the highest accuracy and f1 score by Gaussian RBF SVM given the limited grid search hyperparameter space. However, the training time of plan B is more than twice as long as plan A and C due to its high dimensions and sparsity. Besides, Data in plan A and C may achieve higher accuracy with different hyperparameters.\n",
    "\n",
    "As we broke down the training time of three advanced models, we noticed that SVM models spent more time than Random Forest. One of the reason is that SVM in scikit learn does not support parallel computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For detailed model evaluation, we can use the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T02:50:55.043183Z",
     "iopub.status.busy": "2021-07-03T02:50:55.042903Z",
     "iopub.status.idle": "2021-07-03T02:50:55.055744Z",
     "shell.execute_reply": "2021-07-03T02:50:55.054988Z",
     "shell.execute_reply.started": "2021-07-03T02:50:55.043154Z"
    }
   },
   "outputs": [],
   "source": [
    "def ad_model_eval(clf, X_test, y_test):\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"Mean cross-validated score of the best_estimator: {0:.3f}\".format(clf.best_score_))\n",
    "    print(\"Accuracy on test data: {0:.3f}\".format(clf.score(X_test, y_test)))\n",
    "    print(metrics.classification_report(y_test, y_pred))\n",
    "    print(\"Tuned Model Parameters: {}\".format(clf.best_params_))\n",
    "    skplt.metrics.plot_confusion_matrix(y_test, y_pred, normalize=True)\n",
    "    metrics.plot_roc_curve(clf, X_test, y_test)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T02:50:55.057181Z",
     "iopub.status.busy": "2021-07-03T02:50:55.056804Z",
     "iopub.status.idle": "2021-07-03T02:51:02.141518Z",
     "shell.execute_reply": "2021-07-03T02:51:02.140332Z",
     "shell.execute_reply.started": "2021-07-03T02:50:55.057141Z"
    }
   },
   "outputs": [],
   "source": [
    "ad_model_eval(best_rbf_svc_b, X_test_b, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the detailed model evaluation, Gaussian RBF SVM model trained on data in plan A has a high false positive rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we trained Random Forest models, we can have a look of the feature importance by plotting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T02:51:02.143012Z",
     "iopub.status.busy": "2021-07-03T02:51:02.142632Z",
     "iopub.status.idle": "2021-07-03T02:51:02.148298Z",
     "shell.execute_reply": "2021-07-03T02:51:02.146908Z",
     "shell.execute_reply.started": "2021-07-03T02:51:02.142963Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_names_a = scale_features_a + onehot_features_a\n",
    "feature_names_b = scale_features_b + onehot_features_b\n",
    "feature_names_c = scale_features_a + onehot_features_a + ['cluster_0', 'cluster_1', 'cluster_2', 'cluster_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T02:51:02.150092Z",
     "iopub.status.busy": "2021-07-03T02:51:02.149672Z",
     "iopub.status.idle": "2021-07-03T02:51:02.158933Z",
     "shell.execute_reply": "2021-07-03T02:51:02.158147Z",
     "shell.execute_reply.started": "2021-07-03T02:51:02.150038Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(feature_names_a) == X_train_a.shape[1])\n",
    "print(len(feature_names_b) == X_train_b.shape[1])\n",
    "print(len(feature_names_c) == X_train_c.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T02:51:02.160281Z",
     "iopub.status.busy": "2021-07-03T02:51:02.159894Z",
     "iopub.status.idle": "2021-07-03T02:51:03.083375Z",
     "shell.execute_reply": "2021-07-03T02:51:03.082541Z",
     "shell.execute_reply.started": "2021-07-03T02:51:02.160224Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_importance_a = pd.Series(best_rf_a.best_estimator_.feature_importances_, index=feature_names_a)\n",
    "feature_importance_b = pd.Series(best_rf_b.best_estimator_.feature_importances_, index=feature_names_b)\n",
    "feature_importance_c = pd.Series(best_rf_c.best_estimator_.feature_importances_, index=feature_names_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T02:51:03.084958Z",
     "iopub.status.busy": "2021-07-03T02:51:03.084588Z",
     "iopub.status.idle": "2021-07-03T02:51:03.091234Z",
     "shell.execute_reply": "2021-07-03T02:51:03.090150Z",
     "shell.execute_reply.started": "2021-07-03T02:51:03.084921Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_importance(feature_importance, suffix):\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    feature_importance[feature_importance > 0.015].sort_values().plot.barh(ax=ax)\n",
    "    ax.set_title(\"Feature Importances of Random Forest Model {}\".format(suffix))\n",
    "    ax.set_xlabel(\"Mean decrease in impurity\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T02:51:03.092627Z",
     "iopub.status.busy": "2021-07-03T02:51:03.092363Z",
     "iopub.status.idle": "2021-07-03T02:51:03.454731Z",
     "shell.execute_reply": "2021-07-03T02:51:03.453831Z",
     "shell.execute_reply.started": "2021-07-03T02:51:03.092601Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_importance(feature_importance_a, \"A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T02:51:03.456469Z",
     "iopub.status.busy": "2021-07-03T02:51:03.456034Z",
     "iopub.status.idle": "2021-07-03T02:51:03.710495Z",
     "shell.execute_reply": "2021-07-03T02:51:03.709560Z",
     "shell.execute_reply.started": "2021-07-03T02:51:03.456428Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_importance(feature_importance_b, \"B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T02:51:03.711834Z",
     "iopub.status.busy": "2021-07-03T02:51:03.711587Z",
     "iopub.status.idle": "2021-07-03T02:51:04.007916Z",
     "shell.execute_reply": "2021-07-03T02:51:04.007048Z",
     "shell.execute_reply.started": "2021-07-03T02:51:03.711809Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_importance(feature_importance_c, \"C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set a threshold that only those features whose importance is higher than 0.015 can be in this plot. For data A and C, the feature importance plots are basically the same, and the cluster labels we added by kprototypes clustering does not show up in the feature importance plot which means it is not that important at least in Random Forest model. If we check the top 10 most important features, they are coupon type and occupation type's target and frequency encoding, how frequent to go to coffee houses, education and time's target encoding, income, and age.\n",
    "\n",
    "For data B, we can see that the frequency features: CoffeeHouse, Bar, CarryAway, RestaurantLessThan20, and Restaurant20To50 are on the upper part of the plot, indicating their importances. Numerical features like income, age, temperature, and distance are quite importance as well as the Education level. Coupon features like coupon type and coupon expiration time are also in the plot. Indeed, there are some similarity between data in B and A, C. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we started from data cleanning and exploratory data analysis, built basic and advanced models on target and frequent encoded data, one hot encoded data, and featured expanded data to check which kind of data is best in improving model performance. Finally we reached 76.3% accuracy on test data with Gaussian RBF SVM.\n",
    "\n",
    "The main focus of this notebook is to show various kinds of feature engineering methods applicable on categorical data and the sklearn style workflow to train multiple models at once. For categorical data encoding, we tried one hot encoding, target encoding, and frequency encoding. We also explored one of the feature expansion methods that is suitable for mixed data type: kprototypes clustering. \n",
    "\n",
    "In the next part, we will train GPU supported models like Neural Networks and Gradient Boosting Machines, tune the hyperparamters with Optuna, stack models to get a better predictive performance, and finally try the autoML tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
